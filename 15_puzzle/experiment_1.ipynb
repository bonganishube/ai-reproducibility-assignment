{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd373d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from scipy.stats import norm\n",
    "from collections import deque, defaultdict\n",
    "\n",
    "\n",
    "# --- Environment and Utility Functions ---\n",
    "def encode_15puzzle_state(state):\n",
    "    \"\"\"Encodes the 15-puzzle state into a feature vector\"\"\"\n",
    "    encoded = np.zeros(16 * 2 * 4)\n",
    "    for tile in range(16):\n",
    "        idx = state.index(tile)\n",
    "        row, col = divmod(idx, 4)\n",
    "        encoded[tile * 8 + row] = 1\n",
    "        encoded[tile * 8 + 4 + col] = 1\n",
    "    return encoded\n",
    "\n",
    "\n",
    "def get_valid_moves(state):\n",
    "    \"\"\"Returns all valid moves from the current state\"\"\"\n",
    "    zero_index = state.index(0)\n",
    "    row, col = divmod(zero_index, 4)\n",
    "    valid_moves = []\n",
    "    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    for dr, dc in directions:\n",
    "        new_row, new_col = row + dr, col + dc\n",
    "        if 0 <= new_row < 4 and 0 <= new_col < 4:\n",
    "            new_zero_index = new_row * 4 + new_col\n",
    "            new_state = state[:]\n",
    "            new_state[zero_index], new_state[new_zero_index] = (\n",
    "                new_state[new_zero_index],\n",
    "                new_state[zero_index],\n",
    "            )\n",
    "            valid_moves.append(new_state)\n",
    "    return valid_moves\n",
    "\n",
    "\n",
    "def manhattan_distance(state, goal_state):\n",
    "    \"\"\"Computes Manhattan distance heuristic\"\"\"\n",
    "    total = 0\n",
    "    for i in range(1, 16):\n",
    "        curr_idx = state.index(i)\n",
    "        goal_idx = goal_state.index(i)\n",
    "        curr_row, curr_col = divmod(curr_idx, 4)\n",
    "        goal_row, goal_col = divmod(goal_idx, 4)\n",
    "        total += abs(curr_row - goal_row) + abs(curr_col - goal_col)\n",
    "    return total\n",
    "\n",
    "\n",
    "# --- Neural Network Definitions ---\n",
    "class BayesianLinear(nn.Module):\n",
    "    \"\"\"Bayesian linear layer with local reparameterization\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, prior_mu=0.0, prior_sigma=1.0):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # Initialize parameters with given priors\n",
    "        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.weight_logvar = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
    "        self.bias_logvar = nn.Parameter(torch.empty(out_features))\n",
    "\n",
    "        # Initialize with prior values\n",
    "        nn.init.normal_(self.weight_mu, mean=prior_mu, std=prior_sigma / 10)\n",
    "        nn.init.constant_(self.weight_logvar, math.log(prior_sigma**2))\n",
    "        nn.init.normal_(self.bias_mu, mean=prior_mu, std=prior_sigma / 10)\n",
    "        nn.init.constant_(self.bias_logvar, math.log(prior_sigma**2))\n",
    "\n",
    "        # Prior parameters\n",
    "        self.prior_mu = prior_mu\n",
    "        self.prior_sigma = prior_sigma\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Local reparameterization trick\n",
    "        weight_var = torch.exp(self.weight_logvar)\n",
    "        bias_var = torch.exp(self.bias_logvar)\n",
    "\n",
    "        mu_out = F.linear(x, self.weight_mu, self.bias_mu)\n",
    "        var_out = F.linear(x.pow(2), weight_var, bias_var)\n",
    "\n",
    "        eps = torch.randn_like(mu_out)\n",
    "        return mu_out + torch.sqrt(var_out + 1e-8) * eps\n",
    "\n",
    "    def kl_divergence(self):\n",
    "        \"\"\"Computes KL divergence between posterior and prior\"\"\"\n",
    "        kl_weight = 0.5 * (\n",
    "            (self.weight_mu - self.prior_mu).pow(2) + torch.exp(self.weight_logvar)\n",
    "        ) / (self.prior_sigma**2) - 0.5 * (\n",
    "            1 + self.weight_logvar - math.log(self.prior_sigma**2)\n",
    "        )\n",
    "        kl_bias = 0.5 * (\n",
    "            (self.bias_mu - self.prior_mu).pow(2) + torch.exp(self.bias_logvar)\n",
    "        ) / (self.prior_sigma**2) - 0.5 * (\n",
    "            1 + self.bias_logvar - math.log(self.prior_sigma**2)\n",
    "        )\n",
    "        return kl_weight.sum() + kl_bias.sum()\n",
    "\n",
    "\n",
    "class WUNN(nn.Module):\n",
    "    \"\"\"Weight Uncertainty Neural Network for epistemic uncertainty\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim=20, S=5, prior_mu=0.0, prior_sigma=1.0):\n",
    "        super().__init__()\n",
    "        self.S = S  # Number of samples for forward pass\n",
    "        self.fc1 = BayesianLinear(input_dim, hidden_dim, prior_mu, prior_sigma)\n",
    "        self.fc2 = BayesianLinear(hidden_dim, 1, prior_mu, prior_sigma)\n",
    "\n",
    "    def forward_single(self, x):\n",
    "        \"\"\"Single forward pass\"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "    def predict_sigma_e(self, x, K=100):\n",
    "        \"\"\"Predicts epistemic uncertainty\"\"\"\n",
    "        self.eval()\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            outputs = [self.forward_single(x_tensor).item() for _ in range(K)]\n",
    "        return np.var(outputs)\n",
    "\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    \"\"\"Feedforward Neural Network for aleatoric uncertainty\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim=20, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2_mean = nn.Linear(hidden_dim, 1)\n",
    "        self.fc2_var = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Initialize weights\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "        nn.init.xavier_normal_(self.fc2_mean.weight)\n",
    "        nn.init.xavier_normal_(self.fc2_var.weight)\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.zeros_(self.fc2_mean.bias)\n",
    "        nn.init.zeros_(self.fc2_var.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass returning mean and log variance\"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2_mean(x), self.fc2_var(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Predicts mean and variance\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            mean, logvar = self.forward(x)\n",
    "            return mean.item(), torch.exp(logvar).item()\n",
    "\n",
    "\n",
    "# --- Main Algorithm Implementation ---\n",
    "class LearnHeuristicPrac:\n",
    "    def __init__(self, input_dim, goal_state, params):\n",
    "        # Initialize models with prior parameters\n",
    "        self.nnWUNN = WUNN(\n",
    "            input_dim,\n",
    "            params[\"hidden_dim\"],\n",
    "            prior_mu=params[\"mu0\"],\n",
    "            prior_sigma=math.sqrt(params[\"sigma2_0\"]),\n",
    "        )\n",
    "        self.nnFFNN = FFNN(input_dim, params[\"hidden_dim\"], params[\"dropout_rate\"])        \n",
    "\n",
    "        # Algorithm parameters\n",
    "        self.alpha = params[\"alpha0\"]\n",
    "        self.beta = params[\"beta0\"]\n",
    "        self.epsilon = params[\"epsilon\"]\n",
    "        self.delta = params[\"delta\"]\n",
    "        self.kappa = params[\"kappa\"]\n",
    "        self.gamma = (0.00001 / params[\"beta0\"]) ** (1 / params[\"NumIter\"])  # Key change: Compute γ\n",
    "        self.q = params[\"q\"]\n",
    "        self.K = params[\"K\"]\n",
    "        self.max_steps = params[\"MaxSteps\"]\n",
    "\n",
    "        # Training parameters\n",
    "        self.NumIter = params[\"NumIter\"]\n",
    "        self.NumTasksPerIter = params[\"NumTasksPerIter\"]\n",
    "        self.NumTasksPerIterThresh = params[\"NumTasksPerIterThresh\"]\n",
    "        self.TrainIter = params[\"TrainIter\"]\n",
    "        self.MaxTrainIter = params[\"MaxTrainIter\"]\n",
    "        self.MiniBatchSize = params[\"MiniBatchSize\"]\n",
    "        self.tmax = params[\"tmax\"]\n",
    "\n",
    "        # Memory buffer\n",
    "        self.memoryBuffer = deque(maxlen=params[\"MemoryBufferMaxRecords\"])\n",
    "\n",
    "        # Metrics tracking\n",
    "        self.planner_costs = []\n",
    "        self.optimal_costs = []\n",
    "        self.suboptimalities = []\n",
    "        self.optimality_counts = 0\n",
    "        self.goal_state = goal_state\n",
    "\n",
    "    def h(self, alpha, mu, sigma):\n",
    "        \"\"\"Quantile function of normal distribution\"\"\"\n",
    "        return mu + sigma * norm.ppf(alpha)\n",
    "\n",
    "    def generate_task(self):\n",
    "        \"\"\"Generates a task with high epistemic uncertainty (Algorithm 3)\"\"\"\n",
    "        s_prime = self.goal_state[:]\n",
    "        s_double_prime = None\n",
    "\n",
    "        for _ in range(self.max_steps):\n",
    "            states = {}\n",
    "            valid_moves = get_valid_moves(s_prime)\n",
    "\n",
    "            for s in valid_moves:\n",
    "                if s_double_prime is not None and s == s_double_prime:\n",
    "                    continue\n",
    "\n",
    "                x = encode_15puzzle_state(s)\n",
    "                sigma2_e = self.nnWUNN.predict_sigma_e(x, self.K)\n",
    "                states[tuple(s)] = sigma2_e  # Use tuple as dict key\n",
    "\n",
    "            if not states:\n",
    "                break\n",
    "\n",
    "            # Softmax sampling\n",
    "            states_list = list(states.items())\n",
    "            state_tuples, sigmas = zip(*states_list)\n",
    "            probs = F.softmax(torch.tensor(sigmas), dim=0).numpy()\n",
    "            selected_idx = np.random.choice(len(state_tuples), p=probs)\n",
    "            selected_state = list(state_tuples[selected_idx])\n",
    "            selected_sigma = sigmas[selected_idx]\n",
    "\n",
    "            if selected_sigma >= self.epsilon:\n",
    "                return {\n",
    "                    \"s\": selected_state,\n",
    "                    \"sg\": self.goal_state,\n",
    "                    \"sigma2_e\": selected_sigma,\n",
    "                }\n",
    "\n",
    "            s_double_prime = s_prime\n",
    "            s_prime = selected_state\n",
    "\n",
    "        return None\n",
    "\n",
    "    def ida_star(self, start, goal, heuristic, tmax, start_time):\n",
    "        \"\"\"IDA* implementation with time limit\"\"\"\n",
    "        threshold = heuristic(start)\n",
    "        path = [start]\n",
    "\n",
    "        def search(g, bound):\n",
    "            if time.time() - start_time > tmax:\n",
    "                raise TimeoutError()\n",
    "\n",
    "            node = path[-1]\n",
    "            f = g + heuristic(node)\n",
    "            if f > bound:\n",
    "                return f\n",
    "            if node == goal:\n",
    "                return True\n",
    "\n",
    "            min_t = float(\"inf\")\n",
    "            for neighbor in get_valid_moves(node):\n",
    "                if neighbor in path:\n",
    "                    continue\n",
    "                path.append(neighbor)\n",
    "                t = search(g + 1, bound)\n",
    "                if t is True:\n",
    "                    return True\n",
    "                if t < min_t:\n",
    "                    min_t = t\n",
    "                path.pop()\n",
    "            return min_t\n",
    "\n",
    "        while True:\n",
    "            t = search(0, threshold)\n",
    "            if t is True:\n",
    "                return path\n",
    "            if t == float(\"inf\"):\n",
    "                return None\n",
    "            threshold = t\n",
    "\n",
    "    def uncertainty_aware_heuristic(self, state):\n",
    "        \"\"\"Computes h(s) = max(h(α, ŷ(s), σ_t(s)), 0)\"\"\"\n",
    "        x = encode_15puzzle_state(state)\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        # Get FFNN predictions\n",
    "        self.nnFFNN.eval()\n",
    "        with torch.no_grad():\n",
    "            mean, logvar = self.nnFFNN(x_tensor)\n",
    "            y_hat = mean.item()\n",
    "            sigma2_a = torch.exp(logvar).item()\n",
    "\n",
    "        # Determine which variance to use\n",
    "        sigma2_t = sigma2_a if y_hat < self.yq else self.epsilon\n",
    "\n",
    "        # Compute heuristic value\n",
    "        h_val = self.h(self.alpha, y_hat, math.sqrt(sigma2_t))\n",
    "        return max(h_val, 0)\n",
    "\n",
    "    def compute_metrics(self):\n",
    "        \"\"\"Compute suboptimality and optimality metrics\"\"\"\n",
    "        if not self.planner_costs:\n",
    "            return 0.0, 0.0\n",
    "\n",
    "        # Calculate suboptimality (u_i)\n",
    "        suboptimalities = [\n",
    "            (y / y_star) - 1\n",
    "            for y, y_star in zip(self.planner_costs, self.optimal_costs)\n",
    "            if y_star > 0  # Avoid division by zero\n",
    "        ]\n",
    "        avg_suboptimality = (\n",
    "            sum(suboptimalities) / len(suboptimalities) if suboptimalities else 0.0\n",
    "        )\n",
    "\n",
    "        # Calculate optimality rate (% tasks solved optimally)\n",
    "        optimality_rate = (\n",
    "            (self.optimality_counts / len(self.planner_costs)) * 100\n",
    "            if self.planner_costs\n",
    "            else 0.0\n",
    "        )\n",
    "\n",
    "        return avg_suboptimality, optimality_rate\n",
    "\n",
    "    def train_ffnn(self):\n",
    "        \"\"\"Trains FFNN on entire memory buffer\"\"\"\n",
    "        if len(self.memoryBuffer) < self.MiniBatchSize:\n",
    "            return\n",
    "\n",
    "        optimizer = optim.Adam(self.nnFFNN.parameters())\n",
    "        criterion = nn.GaussianNLLLoss()\n",
    "\n",
    "        # Convert memory buffer to tensors\n",
    "        x_data = torch.stack(\n",
    "            [torch.tensor(x, dtype=torch.float32) for x, _ in self.memoryBuffer]\n",
    "        )\n",
    "        y_data = torch.tensor(\n",
    "            [y for _, y in self.memoryBuffer], dtype=torch.float32\n",
    "        ).unsqueeze(1)\n",
    "\n",
    "        self.nnFFNN.train()\n",
    "        for _ in range(self.TrainIter):\n",
    "            # Shuffle and batch the data\n",
    "            permutation = torch.randperm(len(x_data))\n",
    "            for i in range(0, len(x_data), self.MiniBatchSize):\n",
    "                indices = permutation[i : i + self.MiniBatchSize]\n",
    "                x_batch, y_batch = x_data[indices], y_data[indices]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                mean, logvar = self.nnFFNN(x_batch)\n",
    "                loss = criterion(mean, y_batch, torch.exp(logvar))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    def train_wunn(self):\n",
    "        \"\"\"Trains WUNN with early stopping condition (Algorithm 4 line 29-32)\"\"\"\n",
    "        if len(self.memoryBuffer) < self.MiniBatchSize:\n",
    "            return False\n",
    "\n",
    "        self.nnWUNN.train()\n",
    "        optimizer = optim.Adam(self.nnWUNN.parameters())\n",
    "        early_stop = False\n",
    "\n",
    "        for iter in range(self.MaxTrainIter):\n",
    "            # Compute loss on entire buffer periodically\n",
    "            if iter % 10 == 0:\n",
    "                all_low_uncertainty = True\n",
    "                for x, _ in list(self.memoryBuffer)[\n",
    "                    :100\n",
    "                ]:  # Check subset for efficiency\n",
    "                    sigma2_e = self.nnWUNN.predict_sigma_e(\n",
    "                        x, 10\n",
    "                    )  # Smaller K for faster checking\n",
    "                    if sigma2_e >= self.kappa * self.epsilon:\n",
    "                        all_low_uncertainty = False\n",
    "                        break\n",
    "\n",
    "                if all_low_uncertainty:\n",
    "                    early_stop = True\n",
    "                    break\n",
    "\n",
    "            # Mini-batch training\n",
    "            batch = random.sample(\n",
    "                self.memoryBuffer, min(self.MiniBatchSize, len(self.memoryBuffer))\n",
    "            )\n",
    "            total_loss = 0\n",
    "\n",
    "            for x, y in batch:\n",
    "                x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "                y_tensor = torch.tensor([y], dtype=torch.float32)\n",
    "\n",
    "                # Forward pass with multiple samples\n",
    "                preds = torch.stack(\n",
    "                    [self.nnWUNN.forward_single(x_tensor) for _ in range(self.nnWUNN.S)]\n",
    "                )\n",
    "\n",
    "                # Compute loss\n",
    "                log_likelihood = -F.mse_loss(preds.mean(), y_tensor)\n",
    "                kl_div = (\n",
    "                    self.nnWUNN.fc1.kl_divergence() + self.nnWUNN.fc2.kl_divergence()\n",
    "                )\n",
    "                loss = self.beta * kl_div - log_likelihood\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        return early_stop\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Main learning loop with strict β decay schedule\"\"\"\n",
    "        self.yq = -np.inf\n",
    "\n",
    "        for n in range(1, self.NumIter + 1):\n",
    "            print(f\"\\n=== Iteration {n}/{self.NumIter} ===\")\n",
    "            print(f\"Current β: {self.beta:.6f}\")  # Track β precisely\n",
    "\n",
    "            # Update yq from memory buffer\n",
    "            if self.memoryBuffer:\n",
    "                costs = [y for _, y in self.memoryBuffer]\n",
    "                self.yq = np.quantile(costs, self.q)\n",
    "            print(f\"Current yq (q={self.q}): {self.yq:.2f}, α: {self.alpha:.3f}\")\n",
    "\n",
    "            # Generate and solve tasks\n",
    "            numSolved = 0\n",
    "            for i in range(self.NumTasksPerIter):\n",
    "                T = self.generate_task()\n",
    "                if not T:\n",
    "                    print(\"No task generated (low uncertainty)\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    start_time = time.time()\n",
    "                    plan = self.ida_star(\n",
    "                        T[\"s\"], T[\"sg\"], \n",
    "                        self.uncertainty_aware_heuristic,\n",
    "                        self.tmax, start_time\n",
    "                    )\n",
    "                    if plan:\n",
    "                        numSolved += 1\n",
    "                        plan_cost = len(plan) - 1\n",
    "                        optimal_cost = manhattan_distance(T[\"s\"], T[\"sg\"])\n",
    "                        self.planner_costs.append(plan_cost)\n",
    "                        self.optimal_costs.append(optimal_cost)\n",
    "                        if plan_cost == optimal_cost:\n",
    "                            self.optimality_counts += 1\n",
    "                        print(f\"✓ Task {i+1}: cost={plan_cost}, optimal={optimal_cost}\")\n",
    "                        for state in reversed(plan[:-1]):\n",
    "                            x = encode_15puzzle_state(state)\n",
    "                            y = manhattan_distance(state, T[\"sg\"])\n",
    "                            self.memoryBuffer.appendleft((x, y))\n",
    "                except TimeoutError:\n",
    "                    print(f\"⏳ Task {i+1} timed out\")\n",
    "\n",
    "            # Update α (conditionally)\n",
    "            if numSolved < self.NumTasksPerIterThresh:\n",
    "                self.alpha = max(self.alpha - self.delta, 0.5)\n",
    "                print(f\"Reduced α to {self.alpha:.3f} (solved {numSolved} tasks)\")\n",
    "\n",
    "            # Train models\n",
    "            print(\"Training models...\")\n",
    "            self.train_ffnn()\n",
    "            _ = self.train_wunn()  # early_stop ignored\n",
    "\n",
    "            # Strict β decay (unconditional)\n",
    "            self.beta *= self.gamma\n",
    "            print(f\"Decayed β to {self.beta:.6f} (γ={self.gamma:.6f})\")\n",
    "\n",
    "            # Log metrics\n",
    "            avg_subopt, opt_rate = self.compute_metrics()\n",
    "            print(f\"Iteration {n} results:\")\n",
    "            print(f\"  Solved: {numSolved}/{self.NumTasksPerIter}\")\n",
    "            print(f\"  Suboptimality: {avg_subopt:.3f}\")\n",
    "            print(f\"  Optimality Rate: {opt_rate:.1f}%\")\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    goal_state = list(range(16))\n",
    "    input_dim = len(encode_15puzzle_state(goal_state))\n",
    "\n",
    "    # Algorithm parameters\n",
    "    params = {\n",
    "        \"hidden_dim\": 20,\n",
    "        \"dropout_rate\": 0.1,\n",
    "        \"alpha0\": 0.99,\n",
    "        \"beta0\": 0.05,  # Will decay to 0.00001 in NumIter steps\n",
    "        \"epsilon\": 1.0,\n",
    "        \"delta\": 0.05,\n",
    "        \"kappa\": 0.64,\n",
    "        # gamma is now computed automatically in __init__\n",
    "        \"q\": 0.95,\n",
    "        \"K\": 100,\n",
    "        \"MaxSteps\": 15,\n",
    "        \"mu0\": 0.0,\n",
    "        \"sigma2_0\": 10.0,\n",
    "        \"NumIter\": 50,  # Will decay β from 0.05 to 0.00001 in 50 steps\n",
    "        \"NumTasksPerIter\": 10,\n",
    "        \"NumTasksPerIterThresh\": 6,\n",
    "        \"TrainIter\": 200,\n",
    "        \"MaxTrainIter\": 1000,\n",
    "        \"MiniBatchSize\": 32,\n",
    "        \"tmax\": 30,\n",
    "        \"MemoryBufferMaxRecords\": 25000,\n",
    "    }\n",
    "\n",
    "    learner = LearnHeuristicPrac(input_dim, goal_state, params)\n",
    "    learner.run()\n",
    "\n",
    "    # Final metrics\n",
    "    avg_subopt, opt_rate = learner.compute_metrics()\n",
    "    print(f\"\\n=== Final Metrics ===\")\n",
    "    print(f\"Average Suboptimality: {avg_subopt:.3f}\")\n",
    "    print(f\"Optimality Rate: {opt_rate:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b27d473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from scipy.stats import norm\n",
    "from collections import deque, defaultdict\n",
    "\n",
    "\n",
    "# --- Environment and Utility Functions ---\n",
    "def encode_15puzzle_state(state):\n",
    "    \"\"\"Encodes the 15-puzzle state into a feature vector\"\"\"\n",
    "    encoded = np.zeros(16 * 2 * 4)\n",
    "    for tile in range(16):\n",
    "        idx = state.index(tile)\n",
    "        row, col = divmod(idx, 4)\n",
    "        encoded[tile * 8 + row] = 1\n",
    "        encoded[tile * 8 + 4 + col] = 1\n",
    "    return encoded\n",
    "\n",
    "\n",
    "def get_valid_moves(state):\n",
    "    \"\"\"Returns all valid moves from the current state\"\"\"\n",
    "    zero_index = state.index(0)\n",
    "    row, col = divmod(zero_index, 4)\n",
    "    valid_moves = []\n",
    "    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    for dr, dc in directions:\n",
    "        new_row, new_col = row + dr, col + dc\n",
    "        if 0 <= new_row < 4 and 0 <= new_col < 4:\n",
    "            new_zero_index = new_row * 4 + new_col\n",
    "            new_state = state[:]\n",
    "            new_state[zero_index], new_state[new_zero_index] = (\n",
    "                new_state[new_zero_index],\n",
    "                new_state[zero_index],\n",
    "            )\n",
    "            valid_moves.append(new_state)\n",
    "    return valid_moves\n",
    "\n",
    "\n",
    "def manhattan_distance(state, goal_state):\n",
    "    \"\"\"Computes Manhattan distance heuristic\"\"\"\n",
    "    total = 0\n",
    "    for i in range(1, 16):\n",
    "        curr_idx = state.index(i)\n",
    "        goal_idx = goal_state.index(i)\n",
    "        curr_row, curr_col = divmod(curr_idx, 4)\n",
    "        goal_row, goal_col = divmod(goal_idx, 4)\n",
    "        total += abs(curr_row - goal_row) + abs(curr_col - goal_col)\n",
    "    return total\n",
    "\n",
    "\n",
    "# --- Neural Network Definitions ---\n",
    "class BayesianLinear(nn.Module):\n",
    "    \"\"\"Bayesian linear layer with local reparameterization\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, prior_mu=0.0, prior_sigma=1.0):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # Initialize parameters with given priors\n",
    "        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.weight_logvar = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
    "        self.bias_logvar = nn.Parameter(torch.empty(out_features))\n",
    "\n",
    "        # Initialize with prior values\n",
    "        nn.init.normal_(self.weight_mu, mean=prior_mu, std=prior_sigma / 10)\n",
    "        nn.init.constant_(self.weight_logvar, math.log(prior_sigma**2))\n",
    "        nn.init.normal_(self.bias_mu, mean=prior_mu, std=prior_sigma / 10)\n",
    "        nn.init.constant_(self.bias_logvar, math.log(prior_sigma**2))\n",
    "\n",
    "        # Prior parameters\n",
    "        self.prior_mu = prior_mu\n",
    "        self.prior_sigma = prior_sigma\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Local reparameterization trick\n",
    "        weight_var = torch.exp(self.weight_logvar)\n",
    "        bias_var = torch.exp(self.bias_logvar)\n",
    "\n",
    "        mu_out = F.linear(x, self.weight_mu, self.bias_mu)\n",
    "        var_out = F.linear(x.pow(2), weight_var, bias_var)\n",
    "\n",
    "        eps = torch.randn_like(mu_out)\n",
    "        return mu_out + torch.sqrt(var_out + 1e-8) * eps\n",
    "\n",
    "    def kl_divergence(self):\n",
    "        \"\"\"Computes KL divergence between posterior and prior\"\"\"\n",
    "        kl_weight = 0.5 * (\n",
    "            (self.weight_mu - self.prior_mu).pow(2) + torch.exp(self.weight_logvar)\n",
    "        ) / (self.prior_sigma**2) - 0.5 * (\n",
    "            1 + self.weight_logvar - math.log(self.prior_sigma**2)\n",
    "        )\n",
    "        kl_bias = 0.5 * (\n",
    "            (self.bias_mu - self.prior_mu).pow(2) + torch.exp(self.bias_logvar)\n",
    "        ) / (self.prior_sigma**2) - 0.5 * (\n",
    "            1 + self.bias_logvar - math.log(self.prior_sigma**2)\n",
    "        )\n",
    "        return kl_weight.sum() + kl_bias.sum()\n",
    "\n",
    "\n",
    "class WUNN(nn.Module):\n",
    "    \"\"\"Weight Uncertainty Neural Network for epistemic uncertainty\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim=20, S=5, prior_mu=0.0, prior_sigma=1.0):\n",
    "        super().__init__()\n",
    "        self.S = S  # Number of samples for forward pass\n",
    "        self.fc1 = BayesianLinear(input_dim, hidden_dim, prior_mu, prior_sigma)\n",
    "        self.fc2 = BayesianLinear(hidden_dim, 1, prior_mu, prior_sigma)\n",
    "\n",
    "    def forward_single(self, x):\n",
    "        \"\"\"Single forward pass\"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "    def predict_sigma_e(self, x, K=100):\n",
    "        \"\"\"Predicts epistemic uncertainty\"\"\"\n",
    "        self.eval()\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            outputs = [self.forward_single(x_tensor).item() for _ in range(K)]\n",
    "        return np.var(outputs)\n",
    "\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    \"\"\"Feedforward Neural Network for aleatoric uncertainty\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim=20, dropout_rate=0.025):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2_mean = nn.Linear(hidden_dim, 1)\n",
    "        self.fc2_var = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Initialize weights\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "        nn.init.kaiming_normal_(self.fc2_mean.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "        nn.init.kaiming_normal_(self.fc2_var.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.zeros_(self.fc2_mean.bias)\n",
    "        nn.init.zeros_(self.fc2_var.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass returning mean and log variance\"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2_mean(x), self.fc2_var(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Predicts mean and variance\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            mean, logvar = self.forward(x)\n",
    "            return mean.item(), torch.exp(logvar).item()\n",
    "\n",
    "\n",
    "# --- Main Algorithm Implementation ---\n",
    "class LearnHeuristicPrac:\n",
    "    def __init__(self, input_dim, goal_state, params):\n",
    "        # Initialize models with prior parameters\n",
    "        self.nnWUNN = WUNN(\n",
    "            input_dim,\n",
    "            params[\"hidden_dim\"],\n",
    "            prior_mu=params[\"mu0\"],\n",
    "            prior_sigma=math.sqrt(params[\"sigma2_0\"]),\n",
    "        )\n",
    "        self.nnFFNN = FFNN(input_dim, params[\"hidden_dim\"], params[\"dropout_rate\"])        \n",
    "\n",
    "        # Algorithm parameters\n",
    "        self.alpha = params[\"alpha0\"]\n",
    "        self.beta = params[\"beta0\"]\n",
    "        self.epsilon = params[\"epsilon\"]\n",
    "        self.delta = params[\"delta\"]\n",
    "        self.kappa = params[\"kappa\"]\n",
    "        self.gamma = (0.00001 / params[\"beta0\"]) ** (1 / params[\"NumIter\"])  # Key change: Compute γ\n",
    "        self.q = params[\"q\"]\n",
    "        self.K = params[\"K\"]\n",
    "        self.max_steps = params[\"MaxSteps\"]\n",
    "\n",
    "        # Training parameters\n",
    "        self.NumIter = params[\"NumIter\"]\n",
    "        self.NumTasksPerIter = params[\"NumTasksPerIter\"]\n",
    "        self.NumTasksPerIterThresh = params[\"NumTasksPerIterThresh\"]\n",
    "        self.TrainIter = params[\"TrainIter\"]\n",
    "        self.MaxTrainIter = params[\"MaxTrainIter\"]\n",
    "        self.MiniBatchSize = params[\"MiniBatchSize\"]\n",
    "        self.tmax = params[\"tmax\"]\n",
    "\n",
    "        # Memory buffer\n",
    "        self.memoryBuffer = deque(maxlen=params[\"MemoryBufferMaxRecords\"])\n",
    "\n",
    "        # Metrics tracking\n",
    "        self.planner_costs = []\n",
    "        self.optimal_costs = []\n",
    "        self.suboptimalities = []\n",
    "        self.optimality_counts = 0\n",
    "        self.goal_state = goal_state\n",
    "\n",
    "    def h(self, alpha, mu, sigma):\n",
    "        \"\"\"Quantile function of normal distribution\"\"\"\n",
    "        return mu + sigma * norm.ppf(alpha)\n",
    "\n",
    "    def generate_task(self):\n",
    "        \"\"\"Generates a task with high epistemic uncertainty (Algorithm 3)\"\"\"\n",
    "        s_prime = self.goal_state[:]\n",
    "        s_double_prime = None\n",
    "\n",
    "        for _ in range(self.max_steps):\n",
    "            states = {}\n",
    "            valid_moves = get_valid_moves(s_prime)\n",
    "\n",
    "            for s in valid_moves:\n",
    "                if s_double_prime is not None and s == s_double_prime:\n",
    "                    continue\n",
    "\n",
    "                x = encode_15puzzle_state(s)\n",
    "                sigma2_e = self.nnWUNN.predict_sigma_e(x, self.K)\n",
    "                states[tuple(s)] = sigma2_e  # Use tuple as dict key\n",
    "\n",
    "            if not states:\n",
    "                break\n",
    "\n",
    "            # Softmax sampling\n",
    "            states_list = list(states.items())\n",
    "            state_tuples, sigmas = zip(*states_list)\n",
    "            probs = F.softmax(torch.tensor(sigmas), dim=0).numpy()\n",
    "            selected_idx = np.random.choice(len(state_tuples), p=probs)\n",
    "            selected_state = list(state_tuples[selected_idx])\n",
    "            selected_sigma = sigmas[selected_idx]\n",
    "\n",
    "            if selected_sigma >= self.epsilon:\n",
    "                return {\n",
    "                    \"s\": selected_state,\n",
    "                    \"sg\": self.goal_state,\n",
    "                    \"sigma2_e\": selected_sigma,\n",
    "                }\n",
    "\n",
    "            s_double_prime = s_prime\n",
    "            s_prime = selected_state\n",
    "\n",
    "        return None\n",
    "\n",
    "    def ida_star(self, start, goal, heuristic, tmax, start_time):\n",
    "        \"\"\"IDA* implementation with node counting\"\"\"\n",
    "        threshold = heuristic(start)\n",
    "        path = [start]\n",
    "        total_nodes = 0  # Counter for nodes generated\n",
    "\n",
    "        def search(g, bound):\n",
    "            nonlocal total_nodes\n",
    "            if time.time() - start_time > tmax:\n",
    "                raise TimeoutError()\n",
    "            \n",
    "            node = path[-1]\n",
    "            f = g + heuristic(node)\n",
    "            if f > bound:\n",
    "                return f\n",
    "            if node == goal:\n",
    "                return True\n",
    "            \n",
    "            min_t = float('inf')\n",
    "            neighbors = get_valid_moves(node)\n",
    "            total_nodes += len(neighbors)  # Count all generated nodes\n",
    "            \n",
    "            for neighbor in neighbors:\n",
    "                if neighbor in path:\n",
    "                    continue\n",
    "                path.append(neighbor)\n",
    "                t = search(g + 1, bound)\n",
    "                if t is True:\n",
    "                    return True\n",
    "                if t < min_t:\n",
    "                    min_t = t\n",
    "                path.pop()\n",
    "            return min_t\n",
    "\n",
    "        while True:\n",
    "            t = search(0, threshold)\n",
    "            if t is True:\n",
    "                return path, total_nodes  # Return both path and node count\n",
    "            if t == float('inf'):\n",
    "                return None, total_nodes\n",
    "            threshold = t\n",
    "\n",
    "    def uncertainty_aware_heuristic(self, state):\n",
    "        \"\"\"Computes h(s) = max(h(α, ŷ(s), σ_t(s)), 0)\"\"\"\n",
    "        x = encode_15puzzle_state(state)\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        # Get FFNN predictions\n",
    "        self.nnFFNN.eval()\n",
    "        with torch.no_grad():\n",
    "            mean, logvar = self.nnFFNN(x_tensor)\n",
    "            y_hat = mean.item()\n",
    "            sigma2_a = torch.exp(logvar).item()\n",
    "\n",
    "        # Determine which variance to use\n",
    "        sigma2_t = sigma2_a if y_hat < self.yq else self.epsilon\n",
    "\n",
    "        # Compute heuristic value\n",
    "        h_val = self.h(self.alpha, y_hat, math.sqrt(sigma2_t))\n",
    "        return max(h_val, 0)\n",
    "\n",
    "    def compute_metrics(self):\n",
    "        \"\"\"Compute suboptimality and optimality metrics\"\"\"\n",
    "        if not self.planner_costs:\n",
    "            return 0.0, 0.0\n",
    "\n",
    "        # Calculate suboptimality (u_i)\n",
    "        suboptimalities = [\n",
    "            (y / y_star) - 1\n",
    "            for y, y_star in zip(self.planner_costs, self.optimal_costs)\n",
    "            if y_star > 0  # Avoid division by zero\n",
    "        ]\n",
    "        avg_suboptimality = (\n",
    "            sum(suboptimalities) / len(suboptimalities) if suboptimalities else 0.0\n",
    "        )\n",
    "\n",
    "        # Calculate optimality rate (% tasks solved optimally)\n",
    "        optimality_rate = (\n",
    "            (self.optimality_counts / len(self.planner_costs)) * 100\n",
    "            if self.planner_costs\n",
    "            else 0.0\n",
    "        )\n",
    "\n",
    "        return avg_suboptimality, optimality_rate\n",
    "\n",
    "    def train_ffnn(self):\n",
    "        \"\"\"Trains FFNN on entire memory buffer\"\"\"\n",
    "        if len(self.memoryBuffer) < self.MiniBatchSize:\n",
    "            return\n",
    "\n",
    "        optimizer = optim.Adam(self.nnFFNN.parameters())\n",
    "        criterion = nn.GaussianNLLLoss()\n",
    "\n",
    "        # Convert memory buffer to tensors\n",
    "        x_data = torch.stack(\n",
    "            [torch.tensor(x, dtype=torch.float32) for x, _ in self.memoryBuffer]\n",
    "        )\n",
    "        y_data = torch.tensor(\n",
    "            [y for _, y in self.memoryBuffer], dtype=torch.float32\n",
    "        ).unsqueeze(1)\n",
    "\n",
    "        self.nnFFNN.train()\n",
    "        for _ in range(self.TrainIter):\n",
    "            # Shuffle and batch the data\n",
    "            permutation = torch.randperm(len(x_data))\n",
    "            for i in range(0, len(x_data), self.MiniBatchSize):\n",
    "                indices = permutation[i : i + self.MiniBatchSize]\n",
    "                x_batch, y_batch = x_data[indices], y_data[indices]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                mean, logvar = self.nnFFNN(x_batch)\n",
    "                loss = criterion(mean, y_batch, torch.exp(logvar))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    def train_wunn(self):\n",
    "        \"\"\"Trains WUNN with prioritized sampling and early stopping.\"\"\"\n",
    "        if len(self.memoryBuffer) < self.MiniBatchSize:\n",
    "            return False\n",
    "\n",
    "        self.nnWUNN.train()\n",
    "        optimizer = optim.Adam(self.nnWUNN.parameters(), lr=0.01)\n",
    "        early_stop = False\n",
    "\n",
    "        # Precompute epistemic uncertainties for the entire buffer\n",
    "        uncertainties = []\n",
    "        for x, _ in self.memoryBuffer:\n",
    "            sigma2_e = self.nnWUNN.predict_sigma_e(x, K=10)  # Approximate σ²_e\n",
    "            uncertainties.append(sigma2_e)\n",
    "\n",
    "        # Compute sampling weights\n",
    "        weights = []\n",
    "        for sigma2_e in uncertainties:\n",
    "            if sigma2_e >= self.kappa * self.epsilon:\n",
    "                weight = math.exp(math.sqrt(sigma2_e))  # exp(σ_e)\n",
    "            else:\n",
    "                weight = math.exp(-1)  # C=1\n",
    "            weights.append(weight)\n",
    "\n",
    "        for iter in range(self.MaxTrainIter):\n",
    "            # Early stopping check (unchanged)\n",
    "            if iter % 10 == 0:\n",
    "                all_low_uncertainty = True\n",
    "                for x, _ in list(self.memoryBuffer)[:100]:  # Check subset\n",
    "                    sigma2_e = self.nnWUNN.predict_sigma_e(x, 10)\n",
    "                    if sigma2_e >= self.kappa * self.epsilon:\n",
    "                        all_low_uncertainty = False\n",
    "                        break\n",
    "                if all_low_uncertainty:\n",
    "                    early_stop = True\n",
    "                    break\n",
    "\n",
    "            # --- Prioritized Sampling ---\n",
    "            # Sample indices based on weights\n",
    "            batch_indices = random.choices(\n",
    "                range(len(self.memoryBuffer)),\n",
    "                weights=weights,\n",
    "                k=min(self.MiniBatchSize, len(self.memoryBuffer))\n",
    "            )\n",
    "            batch = [self.memoryBuffer[i] for i in batch_indices]\n",
    "\n",
    "            # Training loop (unchanged)\n",
    "            total_loss = 0\n",
    "            for x, y in batch:\n",
    "                x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "                y_tensor = torch.tensor([y], dtype=torch.float32)\n",
    "                preds = torch.stack(\n",
    "                    [self.nnWUNN.forward_single(x_tensor) for _ in range(self.nnWUNN.S)]\n",
    "                )\n",
    "                log_likelihood = -F.mse_loss(preds.mean(), y_tensor)\n",
    "                kl_div = self.nnWUNN.fc1.kl_divergence() + self.nnWUNN.fc2.kl_divergence()\n",
    "                loss = self.beta * kl_div - log_likelihood\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        return early_stop\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Main learning loop with strict β decay schedule\"\"\"\n",
    "        self.yq = -np.inf\n",
    "\n",
    "        for n in range(1, self.NumIter + 1):\n",
    "            print(f\"\\n=== Iteration {n}/{self.NumIter} ===\")\n",
    "            print(f\"Current β: {self.beta:.6f}\")  # Track β precisely\n",
    "\n",
    "            # Update yq from memory buffer\n",
    "            if self.memoryBuffer:\n",
    "                costs = [y for _, y in self.memoryBuffer]\n",
    "                self.yq = np.quantile(costs, self.q)\n",
    "            print(f\"Current yq (q={self.q}): {self.yq:.2f}, α: {self.alpha:.3f}\")\n",
    "\n",
    "            # Generate and solve tasks\n",
    "            numSolved = 0\n",
    "            for i in range(self.NumTasksPerIter):\n",
    "                T = self.generate_task()\n",
    "                if not T:\n",
    "                    print(\"No task generated (low uncertainty)\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    start_time = time.time()\n",
    "                    plan = self.ida_star(\n",
    "                        T[\"s\"], T[\"sg\"], \n",
    "                        self.uncertainty_aware_heuristic,\n",
    "                        self.tmax, start_time\n",
    "                    )\n",
    "                    if plan:\n",
    "                        numSolved += 1\n",
    "                        plan_cost = len(plan) - 1\n",
    "                        optimal_cost = manhattan_distance(T[\"s\"], T[\"sg\"])\n",
    "                        self.planner_costs.append(plan_cost)\n",
    "                        self.optimal_costs.append(optimal_cost)\n",
    "                        if plan_cost == optimal_cost:\n",
    "                            self.optimality_counts += 1\n",
    "                        print(f\"✓ Task {i+1}: cost={plan_cost}, optimal={optimal_cost}\")\n",
    "                        for state in reversed(plan[:-1]):\n",
    "                            x = encode_15puzzle_state(state)\n",
    "                            y = manhattan_distance(state, T[\"sg\"])\n",
    "                            self.memoryBuffer.appendleft((x, y))\n",
    "                except TimeoutError:\n",
    "                    print(f\"⏳ Task {i+1} timed out\")\n",
    "\n",
    "            # Update α (conditionally)\n",
    "            if numSolved < self.NumTasksPerIterThresh:\n",
    "                self.alpha = max(self.alpha - self.delta, 0.5)\n",
    "                print(f\"Reduced α to {self.alpha:.3f} (solved {numSolved} tasks)\")\n",
    "\n",
    "            # Train models\n",
    "            print(\"Training models...\")\n",
    "            self.train_ffnn()\n",
    "            _ = self.train_wunn()  # early_stop ignored\n",
    "\n",
    "            # Strict β decay (unconditional)\n",
    "            self.beta *= self.gamma\n",
    "            print(f\"Decayed β to {self.beta:.6f} (γ={self.gamma:.6f})\")\n",
    "\n",
    "            # Log metrics\n",
    "            avg_subopt, opt_rate = self.compute_metrics()\n",
    "            print(f\"Iteration {n} results:\")\n",
    "            print(f\"  Solved: {numSolved}/{self.NumTasksPerIter}\")\n",
    "            print(f\"  Suboptimality: {avg_subopt:.3f}\")\n",
    "            print(f\"  Optimality Rate: {opt_rate:.1f}%\")\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    goal_state = list(range(16))\n",
    "    input_dim = len(encode_15puzzle_state(goal_state))\n",
    "\n",
    "    # Algorithm parameters\n",
    "    params = {\n",
    "        \"hidden_dim\": 20,\n",
    "        \"dropout_rate\": 0.025,\n",
    "        \"alpha0\": 0.99,\n",
    "        \"beta0\": 0.05,  # Will decay to 0.00001 in NumIter steps\n",
    "        \"epsilon\": 1.0,\n",
    "        \"delta\": 0.05,\n",
    "        \"kappa\": 0.64,\n",
    "        # gamma is now computed automatically in __init__\n",
    "        \"q\": 0.95,\n",
    "        \"K\": 100,\n",
    "        \"MaxSteps\": 1000,\n",
    "        \"mu0\": 0.0,\n",
    "        \"sigma2_0\": 10.0,\n",
    "        \"NumIter\": 50,  # Will decay β from 0.05 to 0.00001 in 50 steps\n",
    "        \"NumTasksPerIter\": 10,\n",
    "        \"NumTasksPerIterThresh\": 6,\n",
    "        \"TrainIter\": 1000,\n",
    "        \"MaxTrainIter\": 5000,\n",
    "        \"MiniBatchSize\": 100,\n",
    "        \"tmax\": 60,\n",
    "        \"MemoryBufferMaxRecords\": 25000,\n",
    "    }\n",
    "\n",
    "    learner = LearnHeuristicPrac(input_dim, goal_state, params)\n",
    "    learner.run()\n",
    "\n",
    "    # Final metrics\n",
    "    avg_subopt, opt_rate = learner.compute_metrics()\n",
    "    print(f\"\\n=== Final Metrics ===\")\n",
    "    print(f\"Average Suboptimality: {avg_subopt:.3f}\")\n",
    "    print(f\"Optimality Rate: {opt_rate:.1f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
