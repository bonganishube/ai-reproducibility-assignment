{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e42e686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# --- Puzzle Environment Functions ---\n",
    "def encode_15puzzle_state(state):\n",
    "    encoded = np.zeros(16 * 2 * 4)\n",
    "    for tile in range(16):\n",
    "        idx = state.index(tile)\n",
    "        row, col = divmod(idx, 4)\n",
    "        encoded[tile * 8 + row] = 1\n",
    "        encoded[tile * 8 + 4 + col] = 1\n",
    "    return encoded\n",
    "\n",
    "def get_valid_moves(state):\n",
    "    zero_index = state.index(0)\n",
    "    row, col = divmod(zero_index, 4)\n",
    "    valid_moves = []\n",
    "    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    for dr, dc in directions:\n",
    "        new_row, new_col = row + dr, col + dc\n",
    "        if 0 <= new_row < 4 and 0 <= new_col < 4:\n",
    "            new_zero_index = new_row * 4 + new_col\n",
    "            new_state = state[:]\n",
    "            new_state[zero_index], new_state[new_zero_index] = new_state[new_zero_index], new_state[zero_index]\n",
    "            valid_moves.append(new_state)\n",
    "    return valid_moves\n",
    "\n",
    "# --- Bayesian Neural Network ---\n",
    "class BayesianLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, prior_std=2.0):\n",
    "        super().__init__()\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features).normal_(0, 0.1))\n",
    "        self.weight_logvar = nn.Parameter(torch.Tensor(out_features, in_features).fill_(-2))\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(out_features).normal_(0, 0.1))\n",
    "        self.bias_logvar = nn.Parameter(torch.Tensor(out_features).fill_(-2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        weight_std = torch.exp(0.5 * self.weight_logvar)\n",
    "        bias_std = torch.exp(0.5 * self.bias_logvar)\n",
    "        mean = F.linear(x, self.weight_mu, self.bias_mu)\n",
    "        std = torch.sqrt(F.linear(x.pow(2), weight_std.pow(2), bias_std.pow(2)))\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + std * eps\n",
    "\n",
    "    def kl_divergence(self):\n",
    "        kl = 0.5 * (self.weight_mu.pow(2) + torch.exp(self.weight_logvar) - self.weight_logvar - 1).sum()\n",
    "        kl += 0.5 * (self.bias_mu.pow(2) + torch.exp(self.bias_logvar) - self.bias_logvar - 1).sum()\n",
    "        return kl\n",
    "\n",
    "class WUNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=20, S=5, C=1, prior_std=2.0):\n",
    "        super().__init__()\n",
    "        self.S = S\n",
    "        self.C = C\n",
    "        self.fc1 = BayesianLinear(input_dim, hidden_dim, prior_std)\n",
    "        self.fc2 = BayesianLinear(hidden_dim, 1, prior_std)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.01)\n",
    "\n",
    "    def forward_single(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "    def predict_sigma_e(self, x, K):\n",
    "        self.eval()\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            outputs = [self.forward_single(x_tensor).item() for _ in range(K)]\n",
    "        return np.var(outputs)\n",
    "\n",
    "    def elbo_loss(self, x, y, beta=0.05):\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "        y_tensor = torch.tensor([y], dtype=torch.float32)\n",
    "        preds = torch.stack([self.forward_single(x_tensor) for _ in range(self.S)])\n",
    "        log_likelihood = -F.mse_loss(preds.mean(), y_tensor)\n",
    "        kl_div = self.fc1.kl_divergence() + self.fc2.kl_divergence()\n",
    "        return beta * kl_div + (-log_likelihood)\n",
    "\n",
    "    def sample_weighted_batch(self, memory_buffer, batch_size, kappa_epsilon, K):\n",
    "        weights = []\n",
    "        for x, _ in memory_buffer:\n",
    "            sigma2 = self.predict_sigma_e(x, K)\n",
    "            weights.append(sigma2 + 1e-6)  # More numerically stable\n",
    "        weights = np.array(weights)\n",
    "        weights /= weights.sum()\n",
    "        indices = np.random.choice(len(memory_buffer), size=min(batch_size, len(memory_buffer)),\n",
    "                                   replace=False, p=weights)\n",
    "        return [memory_buffer[i] for i in indices]\n",
    "\n",
    "    def train_model(self, memory_buffer, max_iter=500, batch_size=10, kappa_epsilon=0.1, K=5):\n",
    "        self.train()\n",
    "        for iteration in range(max_iter):\n",
    "            batch = self.sample_weighted_batch(memory_buffer, batch_size, kappa_epsilon, K)\n",
    "            for x, y in batch:\n",
    "                loss = self.elbo_loss(x, y)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            if iteration % 100 == 0:\n",
    "                print(f\"WUNN - Iteration [{iteration}/{max_iter}], Loss: {loss.item():.4f}\")\n",
    "            if all(self.predict_sigma_e(x, K=5) < kappa_epsilon for x, _ in memory_buffer):\n",
    "                print(\"Convergence reached. Stopping training.\")\n",
    "                break\n",
    "\n",
    "    \n",
    "# --- Task Generator ---\n",
    "def GenerateTaskPrac(nnWUNN, epsilon, MaxSteps, K, Erev, feature, sg):\n",
    "    s_prime = sg\n",
    "    numSteps = 0\n",
    "    s_prev = None\n",
    "    while numSteps < MaxSteps:\n",
    "        numSteps += 1\n",
    "        states = {}\n",
    "\n",
    "        for s in Erev(s_prime):\n",
    "            if s_prev is not None and s == s_prev:\n",
    "                continue\n",
    "            x = feature(s)\n",
    "            sigma2_e = nnWUNN.predict_sigma_e(x, K)\n",
    "            states[tuple(s)] = sigma2_e\n",
    "            print(f\"Step {numSteps}: Checking state {s} - Epistemic uncertainty (σ²_e) = {sigma2_e:.4f}, Epsilon (ϵ) = {epsilon}\")\n",
    "\n",
    "        if not states:\n",
    "            return None\n",
    "\n",
    "        keys = list(states.keys())\n",
    "        values = np.array([states[k] for k in keys])\n",
    "        probs = np.exp(values) / np.sum(np.exp(values))\n",
    "        s = list(keys[np.random.choice(len(keys), p=probs)])\n",
    "\n",
    "        if states[tuple(s)] >= epsilon:\n",
    "            T = {\n",
    "                \"S\": s,\n",
    "                \"E\": states[tuple(s)],\n",
    "                \"s\": s,\n",
    "                \"sg\": sg\n",
    "            }\n",
    "            print(f\"Task generated with uncertainty σ²_e = {states[tuple(s)]:.4f} ≥ ϵ = {epsilon}\")\n",
    "            return T\n",
    "\n",
    "        s_prev = s_prime\n",
    "        s_prime = s\n",
    "\n",
    "    return None\n",
    "\n",
    "# --- Example Execution ---\n",
    "goal_state = list(range(16))\n",
    "feature = lambda s: np.array(encode_15puzzle_state(s))\n",
    "Erev = get_valid_moves\n",
    "input_dim = len(feature(goal_state))\n",
    "nnWUNN = WUNN(input_dim)\n",
    "\n",
    "# Scrambling utility\n",
    "def scramble(state, steps=10):\n",
    "    s = state[:]\n",
    "    for _ in range(steps):\n",
    "        s = random.choice(get_valid_moves(s))\n",
    "    return s\n",
    "\n",
    "# Value function (distance to goal)\n",
    "def manhattan_distance(state, goal_state):\n",
    "    total = 0\n",
    "    for i in range(1, 16):\n",
    "        curr_idx = state.index(i)\n",
    "        goal_idx = goal_state.index(i)\n",
    "        curr_row, curr_col = divmod(curr_idx, 4)\n",
    "        goal_row, goal_col = divmod(goal_idx, 4)\n",
    "        total += abs(curr_row - goal_row) + abs(curr_col - goal_col)\n",
    "    return total\n",
    "\n",
    "# Create training data\n",
    "memory_buffer = [\n",
    "    (feature(scramble(goal_state, steps=np.random.randint(1, 20))), manhattan_distance(scramble(goal_state, 1), goal_state))\n",
    "    for _ in range(200)\n",
    "]\n",
    "\n",
    "# Train WUNN\n",
    "nnWUNN.train_model(memory_buffer)\n",
    "\n",
    "# Generate task\n",
    "task = GenerateTaskPrac(nnWUNN, epsilon=1, MaxSteps=1000, K=100, Erev=Erev, feature=feature, sg=goal_state)\n",
    "print(\"Generated Task:\", task)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
