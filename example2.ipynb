{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f308e9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from scipy.stats import norm\n",
    "from collections import deque\n",
    "\n",
    "# --- Environment and Utility Functions ---\n",
    "def encode_15puzzle_state(state):\n",
    "    \"\"\"Encodes the 15-puzzle state into a feature vector\"\"\"\n",
    "    encoded = np.zeros(16 * 2 * 4)\n",
    "    for tile in range(16):\n",
    "        idx = state.index(tile)\n",
    "        row, col = divmod(idx, 4)\n",
    "        encoded[tile * 8 + row] = 1\n",
    "        encoded[tile * 8 + 4 + col] = 1\n",
    "    return encoded\n",
    "\n",
    "def get_valid_moves(state):\n",
    "    \"\"\"Returns all valid moves from the current state\"\"\"\n",
    "    zero_index = state.index(0)\n",
    "    row, col = divmod(zero_index, 4)\n",
    "    valid_moves = []\n",
    "    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    for dr, dc in directions:\n",
    "        new_row, new_col = row + dr, col + dc\n",
    "        if 0 <= new_row < 4 and 0 <= new_col < 4:\n",
    "            new_zero_index = new_row * 4 + new_col\n",
    "            new_state = state[:]\n",
    "            new_state[zero_index], new_state[new_zero_index] = new_state[new_zero_index], new_state[zero_index]\n",
    "            valid_moves.append(new_state)\n",
    "    return valid_moves\n",
    "\n",
    "def scramble(state, steps=15):\n",
    "    \"\"\"Scrambles the puzzle by making random moves\"\"\"\n",
    "    s = state[:]\n",
    "    for _ in range(steps):\n",
    "        s = random.choice(get_valid_moves(s))\n",
    "    return s\n",
    "\n",
    "def manhattan_distance(state, goal_state):\n",
    "    \"\"\"Computes Manhattan distance heuristic\"\"\"\n",
    "    total = 0\n",
    "    for i in range(1, 16):\n",
    "        curr_idx = state.index(i)\n",
    "        goal_idx = goal_state.index(i)\n",
    "        curr_row, curr_col = divmod(curr_idx, 4)\n",
    "        goal_row, goal_col = divmod(goal_idx, 4)\n",
    "        total += abs(curr_row - goal_row) + abs(curr_col - goal_col)\n",
    "    return total\n",
    "\n",
    "# --- Neural Network Definitions ---\n",
    "class BayesianLinear(nn.Module):\n",
    "    \"\"\"Bayesian linear layer with local reparameterization\"\"\"\n",
    "    def __init__(self, in_features, out_features, prior_std=1.0):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Initialize parameters properly\n",
    "        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.weight_logvar = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
    "        self.bias_logvar = nn.Parameter(torch.empty(out_features))\n",
    "        \n",
    "        # Initialize with proper values\n",
    "        nn.init.normal_(self.weight_mu, mean=0, std=0.1)\n",
    "        nn.init.constant_(self.weight_logvar, -3)\n",
    "        nn.init.normal_(self.bias_mu, mean=0, std=0.1)\n",
    "        nn.init.constant_(self.bias_logvar, -3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Local reparameterization trick\n",
    "        weight_var = torch.exp(self.weight_logvar)\n",
    "        bias_var = torch.exp(self.bias_logvar)\n",
    "        \n",
    "        mu_out = F.linear(x, self.weight_mu, self.bias_mu)\n",
    "        var_out = F.linear(x.pow(2), weight_var, bias_var)\n",
    "        \n",
    "        eps = torch.randn_like(mu_out)\n",
    "        return mu_out + torch.sqrt(var_out + 1e-8) * eps\n",
    "    \n",
    "    def kl_divergence(self):\n",
    "        \"\"\"Computes KL divergence between posterior and prior\"\"\"\n",
    "        kl = 0.5 * (self.weight_mu.pow(2) + torch.exp(self.weight_logvar) - self.weight_logvar - 1).sum()\n",
    "        kl += 0.5 * (self.bias_mu.pow(2) + torch.exp(self.bias_logvar) - self.bias_logvar - 1).sum()\n",
    "        return kl\n",
    "\n",
    "class WUNN(nn.Module):\n",
    "    \"\"\"Weight Uncertainty Neural Network for epistemic uncertainty\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=20, S=5, prior_std=1.0):\n",
    "        super().__init__()\n",
    "        self.S = S  # Number of samples for forward pass\n",
    "        self.fc1 = BayesianLinear(input_dim, hidden_dim, prior_std)\n",
    "        self.fc2 = BayesianLinear(hidden_dim, 1, prior_std)\n",
    "        \n",
    "    def forward_single(self, x):\n",
    "        \"\"\"Single forward pass\"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "    \n",
    "    def predict_sigma_e(self, x, K=100):\n",
    "        \"\"\"Predicts epistemic uncertainty\"\"\"\n",
    "        self.eval()\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            outputs = [self.forward_single(x_tensor).item() for _ in range(K)]\n",
    "        return np.var(outputs)\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    \"\"\"Feedforward Neural Network for aleatoric uncertainty\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=20, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2_mean = nn.Linear(hidden_dim, 1)\n",
    "        self.fc2_var = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.xavier_normal_(self.fc2_mean.weight)\n",
    "        nn.init.xavier_normal_(self.fc2_var.weight)\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.zeros_(self.fc2_mean.bias)\n",
    "        nn.init.zeros_(self.fc2_var.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass returning mean and log variance\"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2_mean(x), self.fc2_var(x)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"Predicts mean and variance\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            mean, logvar = self.forward(x)\n",
    "            return mean.item(), torch.exp(logvar).item()\n",
    "\n",
    "# --- Main Algorithm Implementation ---\n",
    "class LearnHeuristicPrac:\n",
    "    def __init__(self, input_dim, goal_state, params):\n",
    "        # Initialize models\n",
    "        self.nnWUNN = WUNN(input_dim, params['hidden_dim'])\n",
    "        self.nnFFNN = FFNN(input_dim, params['hidden_dim'], params['dropout_rate'])\n",
    "        \n",
    "        # Algorithm parameters\n",
    "        self.alpha = params['alpha0']\n",
    "        self.beta = params['beta0']\n",
    "        self.epsilon = params['epsilon']\n",
    "        self.delta = params['delta']\n",
    "        self.kappa = params['kappa']\n",
    "        self.gamma = params['gamma']\n",
    "        self.q = params['q']\n",
    "        self.K = params['K']\n",
    "        \n",
    "        # Training parameters\n",
    "        self.NumIter = params['NumIter']\n",
    "        self.NumTasksPerIter = params['NumTasksPerIter']\n",
    "        self.NumTasksPerIterThresh = params['NumTasksPerIterThresh']\n",
    "        self.TrainIter = params['TrainIter']\n",
    "        self.MaxTrainIter = params['MaxTrainIter']\n",
    "        self.MiniBatchSize = params['MiniBatchSize']\n",
    "        self.tmax = params['tmax']\n",
    "        \n",
    "        # Memory buffer (using deque for efficient trimming)\n",
    "        self.memoryBuffer = deque(maxlen=params['MemoryBufferMaxRecords'])\n",
    "        \n",
    "        # Metrics tracking\n",
    "        self.planner_costs = []\n",
    "        self.optimal_costs = []\n",
    "        self.suboptimalities = []\n",
    "        self.optimality_counts = 0\n",
    "        self.goal_state = goal_state\n",
    "        \n",
    "    def h(self, alpha, mu, sigma):\n",
    "        \"\"\"Quantile function of normal distribution\"\"\"\n",
    "        return mu + sigma * norm.ppf(alpha)\n",
    "    \n",
    "    def generate_task(self):\n",
    "        \"\"\"Generates a task with high epistemic uncertainty\"\"\"\n",
    "        start_state = scramble(self.goal_state)\n",
    "        x = encode_15puzzle_state(start_state)\n",
    "        sigma2_e = self.nnWUNN.predict_sigma_e(x, self.K)\n",
    "        \n",
    "        if sigma2_e >= self.epsilon:\n",
    "            return {\n",
    "                's': start_state,\n",
    "                'sg': self.goal_state,\n",
    "                'sigma2_e': sigma2_e\n",
    "            }\n",
    "        return None\n",
    "    \n",
    "    def ida_star(self, start, goal, heuristic, tmax, start_time):\n",
    "        \"\"\"IDA* implementation with time limit\"\"\"\n",
    "        threshold = heuristic(start)\n",
    "        path = [start]\n",
    "        \n",
    "        def search(g, bound):\n",
    "            if time.time() - start_time > tmax:\n",
    "                raise TimeoutError()\n",
    "            \n",
    "            node = path[-1]\n",
    "            f = g + heuristic(node)\n",
    "            if f > bound:\n",
    "                return f\n",
    "            if node == goal:\n",
    "                return True\n",
    "            \n",
    "            min_t = float('inf')\n",
    "            for neighbor in get_valid_moves(node):\n",
    "                if neighbor in path:\n",
    "                    continue\n",
    "                path.append(neighbor)\n",
    "                t = search(g + 1, bound)\n",
    "                if t is True:\n",
    "                    return True\n",
    "                if t < min_t:\n",
    "                    min_t = t\n",
    "                path.pop()\n",
    "            return min_t\n",
    "        \n",
    "        while True:\n",
    "            t = search(0, threshold)\n",
    "            if t is True:\n",
    "                return path\n",
    "            if t == float('inf'):\n",
    "                return None\n",
    "            threshold = t\n",
    "    \n",
    "    def uncertainty_aware_heuristic(self, state):\n",
    "        \"\"\"Computes h(s) = max(h(α, ŷ(s), σ_t(s)), 0)\"\"\"\n",
    "        x = encode_15puzzle_state(state)\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        # Get FFNN predictions\n",
    "        self.nnFFNN.eval()\n",
    "        with torch.no_grad():\n",
    "            mean, logvar = self.nnFFNN(x_tensor)\n",
    "            y_hat = mean.item()\n",
    "            sigma2_a = torch.exp(logvar).item()\n",
    "        \n",
    "        # Determine which variance to use\n",
    "        sigma2_t = sigma2_a if y_hat < self.yq else self.epsilon\n",
    "        \n",
    "        # Compute heuristic value\n",
    "        h_val = self.h(self.alpha, y_hat, math.sqrt(sigma2_t))\n",
    "        return max(h_val, 0)\n",
    "    \n",
    "    def compute_metrics(self):\n",
    "        \"\"\"Compute suboptimality and optimality metrics\"\"\"\n",
    "        if not self.planner_costs:\n",
    "            return 0.0, 0.0\n",
    "        \n",
    "        # Calculate suboptimality (u_i)\n",
    "        suboptimalities = [\n",
    "            (y / y_star) - 1 \n",
    "            for y, y_star in zip(self.planner_costs, self.optimal_costs)\n",
    "            if y_star > 0  # Avoid division by zero\n",
    "        ]\n",
    "        avg_suboptimality = sum(suboptimalities) / len(suboptimalities) if suboptimalities else 0.0\n",
    "\n",
    "        # Calculate optimality rate (% tasks solved optimally)\n",
    "        optimality_rate = (self.optimality_counts / len(self.planner_costs)) * 100 if self.planner_costs else 0.0\n",
    "\n",
    "        return avg_suboptimality, optimality_rate\n",
    "    \n",
    "    def train_ffnn(self):\n",
    "        \"\"\"Trains FFNN on entire memory buffer\"\"\"\n",
    "        if len(self.memoryBuffer) < self.MiniBatchSize:\n",
    "            return\n",
    "        \n",
    "        optimizer = optim.Adam(self.nnFFNN.parameters())\n",
    "        criterion = nn.GaussianNLLLoss()\n",
    "        \n",
    "        # Convert memory buffer to tensors\n",
    "        x_data = torch.stack([torch.tensor(x, dtype=torch.float32) for x, _ in self.memoryBuffer])\n",
    "        y_data = torch.tensor([y for _, y in self.memoryBuffer], dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "        self.nnFFNN.train()\n",
    "        for _ in range(self.TrainIter):\n",
    "            # Shuffle and batch the data\n",
    "            permutation = torch.randperm(len(x_data))\n",
    "            for i in range(0, len(x_data), self.MiniBatchSize):\n",
    "                indices = permutation[i:i+self.MiniBatchSize]\n",
    "                x_batch, y_batch = x_data[indices], y_data[indices]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                mean, logvar = self.nnFFNN(x_batch)\n",
    "                loss = criterion(mean, y_batch, torch.exp(logvar))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "    def train_wunn(self):\n",
    "        \"\"\"Trains WUNN with early stopping condition\"\"\"\n",
    "        if len(self.memoryBuffer) < self.MiniBatchSize:\n",
    "            return False\n",
    "        \n",
    "        self.nnWUNN.train()\n",
    "        optimizer = optim.Adam(self.nnWUNN.parameters())\n",
    "        \n",
    "        early_stop = False\n",
    "        for iter in range(self.MaxTrainIter):\n",
    "            # Mini-batch training\n",
    "            batch = random.sample(self.memoryBuffer, self.MiniBatchSize)\n",
    "            total_loss = 0\n",
    "            \n",
    "            for x, y in batch:\n",
    "                x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "                y_tensor = torch.tensor([y], dtype=torch.float32)\n",
    "                \n",
    "                # Forward pass with multiple samples\n",
    "                preds = torch.stack([self.nnWUNN.forward_single(x_tensor) for _ in range(self.nnWUNN.S)])\n",
    "                \n",
    "                # Compute loss\n",
    "                log_likelihood = -F.mse_loss(preds.mean(), y_tensor)\n",
    "                kl_div = self.nnWUNN.fc1.kl_divergence() + self.nnWUNN.fc2.kl_divergence()\n",
    "                loss = self.beta * kl_div - log_likelihood\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            # Check early stopping condition periodically\n",
    "            if iter % 10 == 0:\n",
    "                self.nnWUNN.eval()\n",
    "                all_low_uncertainty = True\n",
    "                \n",
    "                # Check a subset of the buffer for efficiency\n",
    "                check_samples = min(100, len(self.memoryBuffer))\n",
    "                for x, _ in random.sample(self.memoryBuffer, check_samples):\n",
    "                    sigma2_e = self.nnWUNN.predict_sigma_e(x, 10)  # Smaller K for faster checking\n",
    "                    if sigma2_e >= self.kappa * self.epsilon:\n",
    "                        all_low_uncertainty = False\n",
    "                        break\n",
    "                \n",
    "                if all_low_uncertainty:\n",
    "                    print(f\"WUNN training stopped early (iteration {iter})\")\n",
    "                    early_stop = True\n",
    "                    break\n",
    "                \n",
    "                self.nnWUNN.train()\n",
    "        \n",
    "        return early_stop\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Main learning loop\"\"\"\n",
    "        # Initialize yq\n",
    "        self.yq = -np.inf\n",
    "        \n",
    "        for n in range(1, self.NumIter + 1):\n",
    "            print(f\"\\n=== Iteration {n}/{self.NumIter} ===\")\n",
    "            \n",
    "            # Update yq from memory buffer\n",
    "            if self.memoryBuffer:\n",
    "                costs = [y for _, y in self.memoryBuffer]\n",
    "                self.yq = np.quantile(costs, self.q)\n",
    "            print(f\"Current yq (q={self.q} quantile): {self.yq:.2f}, α: {self.alpha:.3f}, β: {self.beta:.3f}\")\n",
    "            \n",
    "            # Generate and solve tasks\n",
    "            numSolved = 0\n",
    "            for i in range(self.NumTasksPerIter):\n",
    "                T = self.generate_task()\n",
    "                if T is None:\n",
    "                    print(\"No task generated (low uncertainty)\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    start_time = time.time()\n",
    "                    plan = self.ida_star(\n",
    "                        T['s'],\n",
    "                        T['sg'],\n",
    "                        self.uncertainty_aware_heuristic,\n",
    "                        self.tmax,\n",
    "                        start_time\n",
    "                    )\n",
    "                    \n",
    "                    if plan:\n",
    "                        numSolved += 1\n",
    "                        plan_cost = len(plan) - 1  # Cost = steps taken\n",
    "                        optimal_cost = manhattan_distance(T['s'], T['sg'])\n",
    "                        \n",
    "                        # Record costs for metrics\n",
    "                        self.planner_costs.append(plan_cost)\n",
    "                        self.optimal_costs.append(optimal_cost)\n",
    "                        \n",
    "                        # Track optimal solutions\n",
    "                        if plan_cost == optimal_cost:\n",
    "                            self.optimality_counts += 1\n",
    "                        \n",
    "                        print(f\"✓ Solved task {i+1} (cost: {plan_cost}, optimal: {optimal_cost})\")\n",
    "                        \n",
    "                        # Add to memory buffer (most recent first)\n",
    "                        for state in reversed(plan[:-1]):  # Exclude goal state, reversed for recent first\n",
    "                            x = encode_15puzzle_state(state)\n",
    "                            y = manhattan_distance(state, T['sg'])\n",
    "                            self.memoryBuffer.appendleft((x, y))  # Add to front\n",
    "                \n",
    "                except TimeoutError:\n",
    "                    print(f\"⏳ Task {i+1} timed out\")\n",
    "                    continue\n",
    "            \n",
    "            # Update α and β based on solved tasks\n",
    "            if numSolved < self.NumTasksPerIterThresh:\n",
    "                self.alpha = max(self.alpha - self.delta, 0.5)\n",
    "                self.updateBeta = False\n",
    "                print(f\"Reduced α to {self.alpha:.3f} (only solved {numSolved} tasks)\")\n",
    "            else:\n",
    "                self.updateBeta = True\n",
    "            \n",
    "            # Train models\n",
    "            print(\"Training models...\")\n",
    "            self.train_ffnn()\n",
    "            early_stop = self.train_wunn()\n",
    "            \n",
    "            # Update β if conditions met\n",
    "            if self.updateBeta and not early_stop:\n",
    "                self.beta *= self.gamma\n",
    "                print(f\"Reduced β to {self.beta:.3f}\")\n",
    "            \n",
    "            # Compute and log metrics\n",
    "            avg_subopt, opt_rate = self.compute_metrics()\n",
    "            print(f\"Iteration {n} complete:\")\n",
    "            print(f\"  Solved {numSolved}/{self.NumTasksPerIter} tasks\")\n",
    "            print(f\"  Avg Suboptimality: {avg_subopt:.3f}\")\n",
    "            print(f\"  Optimality Rate: {opt_rate:.1f}%\")\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Define goal state and feature dimension\n",
    "    goal_state = list(range(16))\n",
    "    input_dim = len(encode_15puzzle_state(goal_state))\n",
    "    \n",
    "    # Algorithm parameters\n",
    "    params = {\n",
    "        'hidden_dim': 20,\n",
    "        'dropout_rate': 0.1,\n",
    "        'alpha0': 0.99,\n",
    "        'beta0': 0.05,\n",
    "        'epsilon': 1.0,\n",
    "        'delta': 0.01,\n",
    "        'kappa': 0.5,\n",
    "        'gamma': 0.9,\n",
    "        'q': 0.95,\n",
    "        'K': 50,\n",
    "        'NumIter': 30,\n",
    "        'NumTasksPerIter': 25,\n",
    "        'NumTasksPerIterThresh': 3,\n",
    "        'TrainIter': 200,\n",
    "        'MaxTrainIter': 1000,\n",
    "        'MiniBatchSize': 32,\n",
    "        'tmax': 30,  # seconds per task\n",
    "        'MemoryBufferMaxRecords': 5000\n",
    "    }\n",
    "    \n",
    "    # Initialize and run the algorithm\n",
    "    learner = LearnHeuristicPrac(input_dim, goal_state, params)\n",
    "    learner.run()\n",
    "\n",
    "    # Final metrics\n",
    "    avg_subopt, opt_rate = learner.compute_metrics()\n",
    "    print(f\"\\n=== Final Metrics ===\")\n",
    "    print(f\"Average Suboptimality: {avg_subopt:.3f}\")\n",
    "    print(f\"Optimality Rate: {opt_rate:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
