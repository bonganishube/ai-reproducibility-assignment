{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f308e9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Iteration 1/30 ===\n",
      "Current yq (q=0.95 quantile): -inf, α: 0.990, β: 0.050\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 463\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;66;03m# Initialize and run the algorithm\u001b[39;00m\n\u001b[32m    462\u001b[39m learner = LearnHeuristicPrac(input_dim, goal_state, params)\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m \u001b[43mlearner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[38;5;66;03m# Final metrics\u001b[39;00m\n\u001b[32m    466\u001b[39m avg_subopt, opt_rate = learner.compute_metrics()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 375\u001b[39m, in \u001b[36mLearnHeuristicPrac.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    374\u001b[39m     start_time = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m     plan = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mida_star\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m        \u001b[49m\u001b[43mT\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m        \u001b[49m\u001b[43mT\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msg\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43muncertainty_aware_heuristic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    383\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m plan:\n\u001b[32m    384\u001b[39m         numSolved += \u001b[32m1\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 228\u001b[39m, in \u001b[36mLearnHeuristicPrac.ida_star\u001b[39m\u001b[34m(self, start, goal, heuristic, tmax, start_time)\u001b[39m\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m min_t\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m     t = \u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    230\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 219\u001b[39m, in \u001b[36mLearnHeuristicPrac.ida_star.<locals>.search\u001b[39m\u001b[34m(g, bound)\u001b[39m\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    218\u001b[39m path.append(neighbor)\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m t = \u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 219\u001b[39m, in \u001b[36mLearnHeuristicPrac.ida_star.<locals>.search\u001b[39m\u001b[34m(g, bound)\u001b[39m\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    218\u001b[39m path.append(neighbor)\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m t = \u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: LearnHeuristicPrac.ida_star.<locals>.search at line 219 (3 times)]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 219\u001b[39m, in \u001b[36mLearnHeuristicPrac.ida_star.<locals>.search\u001b[39m\u001b[34m(g, bound)\u001b[39m\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    218\u001b[39m path.append(neighbor)\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m t = \u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 208\u001b[39m, in \u001b[36mLearnHeuristicPrac.ida_star.<locals>.search\u001b[39m\u001b[34m(g, bound)\u001b[39m\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n\u001b[32m    207\u001b[39m node = path[-\u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m f = g + \u001b[43mheuristic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m f > bound:\n\u001b[32m    210\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 251\u001b[39m, in \u001b[36mLearnHeuristicPrac.uncertainty_aware_heuristic\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m    248\u001b[39m sigma2_t = sigma2_a \u001b[38;5;28;01mif\u001b[39;00m y_hat < \u001b[38;5;28mself\u001b[39m.yq \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.epsilon\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# Compute heuristic value\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m h_val = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43msigma2_t\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(h_val, \u001b[32m0\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 182\u001b[39m, in \u001b[36mLearnHeuristicPrac.h\u001b[39m\u001b[34m(self, alpha, mu, sigma)\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mh\u001b[39m(\u001b[38;5;28mself\u001b[39m, alpha, mu, sigma):\n\u001b[32m    181\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Quantile function of normal distribution\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m mu + sigma * \u001b[43mnorm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mppf\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/scipy/stats/_distn_infrastructure.py:2306\u001b[39m, in \u001b[36mrv_continuous.ppf\u001b[39m\u001b[34m(self, q, *args, **kwds)\u001b[39m\n\u001b[32m   2304\u001b[39m lower_bound = _a * scale + loc\n\u001b[32m   2305\u001b[39m upper_bound = _b * scale + loc\n\u001b[32m-> \u001b[39m\u001b[32m2306\u001b[39m place(output, cond2, \u001b[43margsreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcond2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower_bound\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m])\n\u001b[32m   2307\u001b[39m place(output, cond3, argsreduce(cond3, upper_bound)[\u001b[32m0\u001b[39m])\n\u001b[32m   2309\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.any(cond):  \u001b[38;5;66;03m# call only if at least 1 entry\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/scipy/stats/_distn_infrastructure.py:643\u001b[39m, in \u001b[36margsreduce\u001b[39m\u001b[34m(cond, *args)\u001b[39m\n\u001b[32m    640\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(newargs, (\u001b[38;5;28mlist\u001b[39m | \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m    641\u001b[39m     newargs = (newargs,)\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcond\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    644\u001b[39m     \u001b[38;5;66;03m# broadcast arrays with cond\u001b[39;00m\n\u001b[32m    645\u001b[39m     *newargs, cond = np.broadcast_arrays(*newargs, cond)\n\u001b[32m    646\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [arg.ravel() \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m newargs]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:2700\u001b[39m, in \u001b[36mall\u001b[39m\u001b[34m(a, axis, out, keepdims, where)\u001b[39m\n\u001b[32m   2611\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_all_dispatcher)\n\u001b[32m   2612\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mall\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=np._NoValue, *, where=np._NoValue):\n\u001b[32m   2613\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2614\u001b[39m \u001b[33;03m    Test whether all array elements along a given axis evaluate to True.\u001b[39;00m\n\u001b[32m   2615\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2698\u001b[39m \n\u001b[32m   2699\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2700\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction_any_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogical_and\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mall\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2701\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:100\u001b[39m, in \u001b[36m_wrapreduction_any_all\u001b[39m\u001b[34m(obj, ufunc, method, axis, out, **kwargs)\u001b[39m\n\u001b[32m     98\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     99\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc.reduce(obj, axis, \u001b[38;5;28mbool\u001b[39m, out, **passkwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/numpy/_core/_methods.py:74\u001b[39m, in \u001b[36m_all\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, where)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# Parsing keyword arguments is currently fairly slow, so avoid it for now\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m where \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m umr_all(a, axis, dtype, out, keepdims, where=where)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from scipy.stats import norm\n",
    "from collections import deque\n",
    "\n",
    "# --- Environment and Utility Functions ---\n",
    "def encode_15puzzle_state(state):\n",
    "    \"\"\"Encodes the 15-puzzle state into a feature vector\"\"\"\n",
    "    encoded = np.zeros(16 * 2 * 4)\n",
    "    for tile in range(16):\n",
    "        idx = state.index(tile)\n",
    "        row, col = divmod(idx, 4)\n",
    "        encoded[tile * 8 + row] = 1\n",
    "        encoded[tile * 8 + 4 + col] = 1\n",
    "    return encoded\n",
    "\n",
    "def get_valid_moves(state):\n",
    "    \"\"\"Returns all valid moves from the current state\"\"\"\n",
    "    zero_index = state.index(0)\n",
    "    row, col = divmod(zero_index, 4)\n",
    "    valid_moves = []\n",
    "    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    for dr, dc in directions:\n",
    "        new_row, new_col = row + dr, col + dc\n",
    "        if 0 <= new_row < 4 and 0 <= new_col < 4:\n",
    "            new_zero_index = new_row * 4 + new_col\n",
    "            new_state = state[:]\n",
    "            new_state[zero_index], new_state[new_zero_index] = new_state[new_zero_index], new_state[zero_index]\n",
    "            valid_moves.append(new_state)\n",
    "    return valid_moves\n",
    "\n",
    "def scramble(state, steps=15):\n",
    "    \"\"\"Scrambles the puzzle by making random moves\"\"\"\n",
    "    s = state[:]\n",
    "    for _ in range(steps):\n",
    "        s = random.choice(get_valid_moves(s))\n",
    "    return s\n",
    "\n",
    "def manhattan_distance(state, goal_state):\n",
    "    \"\"\"Computes Manhattan distance heuristic\"\"\"\n",
    "    total = 0\n",
    "    for i in range(1, 16):\n",
    "        curr_idx = state.index(i)\n",
    "        goal_idx = goal_state.index(i)\n",
    "        curr_row, curr_col = divmod(curr_idx, 4)\n",
    "        goal_row, goal_col = divmod(goal_idx, 4)\n",
    "        total += abs(curr_row - goal_row) + abs(curr_col - goal_col)\n",
    "    return total\n",
    "\n",
    "# --- Neural Network Definitions ---\n",
    "class BayesianLinear(nn.Module):\n",
    "    \"\"\"Bayesian linear layer with local reparameterization\"\"\"\n",
    "    def __init__(self, in_features, out_features, prior_std=1.0):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Initialize parameters properly\n",
    "        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.weight_logvar = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
    "        self.bias_logvar = nn.Parameter(torch.empty(out_features))\n",
    "        \n",
    "        # Initialize with proper values\n",
    "        nn.init.normal_(self.weight_mu, mean=0, std=0.1)\n",
    "        nn.init.constant_(self.weight_logvar, -3)\n",
    "        nn.init.normal_(self.bias_mu, mean=0, std=0.1)\n",
    "        nn.init.constant_(self.bias_logvar, -3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Local reparameterization trick\n",
    "        weight_var = torch.exp(self.weight_logvar)\n",
    "        bias_var = torch.exp(self.bias_logvar)\n",
    "        \n",
    "        mu_out = F.linear(x, self.weight_mu, self.bias_mu)\n",
    "        var_out = F.linear(x.pow(2), weight_var, bias_var)\n",
    "        \n",
    "        eps = torch.randn_like(mu_out)\n",
    "        return mu_out + torch.sqrt(var_out + 1e-8) * eps\n",
    "    \n",
    "    def kl_divergence(self):\n",
    "        \"\"\"Computes KL divergence between posterior and prior\"\"\"\n",
    "        kl = 0.5 * (self.weight_mu.pow(2) + torch.exp(self.weight_logvar) - self.weight_logvar - 1).sum()\n",
    "        kl += 0.5 * (self.bias_mu.pow(2) + torch.exp(self.bias_logvar) - self.bias_logvar - 1).sum()\n",
    "        return kl\n",
    "\n",
    "class WUNN(nn.Module):\n",
    "    \"\"\"Weight Uncertainty Neural Network for epistemic uncertainty\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=20, S=5, prior_std=1.0):\n",
    "        super().__init__()\n",
    "        self.S = S  # Number of samples for forward pass\n",
    "        self.fc1 = BayesianLinear(input_dim, hidden_dim, prior_std)\n",
    "        self.fc2 = BayesianLinear(hidden_dim, 1, prior_std)\n",
    "        \n",
    "    def forward_single(self, x):\n",
    "        \"\"\"Single forward pass\"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "    \n",
    "    def predict_sigma_e(self, x, K=100):\n",
    "        \"\"\"Predicts epistemic uncertainty\"\"\"\n",
    "        self.eval()\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            outputs = [self.forward_single(x_tensor).item() for _ in range(K)]\n",
    "        return np.var(outputs)\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    \"\"\"Feedforward Neural Network for aleatoric uncertainty\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=20, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2_mean = nn.Linear(hidden_dim, 1)\n",
    "        self.fc2_var = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.xavier_normal_(self.fc2_mean.weight)\n",
    "        nn.init.xavier_normal_(self.fc2_var.weight)\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.zeros_(self.fc2_mean.bias)\n",
    "        nn.init.zeros_(self.fc2_var.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass returning mean and log variance\"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2_mean(x), self.fc2_var(x)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"Predicts mean and variance\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            mean, logvar = self.forward(x)\n",
    "            return mean.item(), torch.exp(logvar).item()\n",
    "\n",
    "# --- Main Algorithm Implementation ---\n",
    "class LearnHeuristicPrac:\n",
    "    def __init__(self, input_dim, goal_state, params):\n",
    "        # Initialize models\n",
    "        self.nnWUNN = WUNN(input_dim, params['hidden_dim'])\n",
    "        self.nnFFNN = FFNN(input_dim, params['hidden_dim'], params['dropout_rate'])\n",
    "        \n",
    "        # Algorithm parameters\n",
    "        self.alpha = params['alpha0']\n",
    "        self.beta = params['beta0']\n",
    "        self.epsilon = params['epsilon']\n",
    "        self.delta = params['delta']\n",
    "        self.kappa = params['kappa']\n",
    "        self.gamma = params['gamma']\n",
    "        self.q = params['q']\n",
    "        self.K = params['K']\n",
    "        \n",
    "        # Training parameters\n",
    "        self.NumIter = params['NumIter']\n",
    "        self.NumTasksPerIter = params['NumTasksPerIter']\n",
    "        self.NumTasksPerIterThresh = params['NumTasksPerIterThresh']\n",
    "        self.TrainIter = params['TrainIter']\n",
    "        self.MaxTrainIter = params['MaxTrainIter']\n",
    "        self.MiniBatchSize = params['MiniBatchSize']\n",
    "        self.tmax = params['tmax']\n",
    "        \n",
    "        # Memory buffer (using deque for efficient trimming)\n",
    "        self.memoryBuffer = deque(maxlen=params['MemoryBufferMaxRecords'])\n",
    "        \n",
    "        # Metrics tracking\n",
    "        self.planner_costs = []\n",
    "        self.optimal_costs = []\n",
    "        self.suboptimalities = []\n",
    "        self.optimality_counts = 0\n",
    "        self.goal_state = goal_state\n",
    "        \n",
    "    def h(self, alpha, mu, sigma):\n",
    "        \"\"\"Quantile function of normal distribution\"\"\"\n",
    "        return mu + sigma * norm.ppf(alpha)\n",
    "    \n",
    "    def generate_task(self):\n",
    "        \"\"\"Generates a task with high epistemic uncertainty\"\"\"\n",
    "        start_state = scramble(self.goal_state)\n",
    "        x = encode_15puzzle_state(start_state)\n",
    "        sigma2_e = self.nnWUNN.predict_sigma_e(x, self.K)\n",
    "        \n",
    "        if sigma2_e >= self.epsilon:\n",
    "            return {\n",
    "                's': start_state,\n",
    "                'sg': self.goal_state,\n",
    "                'sigma2_e': sigma2_e\n",
    "            }\n",
    "        return None\n",
    "    \n",
    "    def ida_star(self, start, goal, heuristic, tmax, start_time):\n",
    "        \"\"\"IDA* implementation with time limit\"\"\"\n",
    "        threshold = heuristic(start)\n",
    "        path = [start]\n",
    "        \n",
    "        def search(g, bound):\n",
    "            if time.time() - start_time > tmax:\n",
    "                raise TimeoutError()\n",
    "            \n",
    "            node = path[-1]\n",
    "            f = g + heuristic(node)\n",
    "            if f > bound:\n",
    "                return f\n",
    "            if node == goal:\n",
    "                return True\n",
    "            \n",
    "            min_t = float('inf')\n",
    "            for neighbor in get_valid_moves(node):\n",
    "                if neighbor in path:\n",
    "                    continue\n",
    "                path.append(neighbor)\n",
    "                t = search(g + 1, bound)\n",
    "                if t is True:\n",
    "                    return True\n",
    "                if t < min_t:\n",
    "                    min_t = t\n",
    "                path.pop()\n",
    "            return min_t\n",
    "        \n",
    "        while True:\n",
    "            t = search(0, threshold)\n",
    "            if t is True:\n",
    "                return path\n",
    "            if t == float('inf'):\n",
    "                return None\n",
    "            threshold = t\n",
    "    \n",
    "    def uncertainty_aware_heuristic(self, state):\n",
    "        \"\"\"Computes h(s) = max(h(α, ŷ(s), σ_t(s)), 0)\"\"\"\n",
    "        x = encode_15puzzle_state(state)\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        # Get FFNN predictions\n",
    "        self.nnFFNN.eval()\n",
    "        with torch.no_grad():\n",
    "            mean, logvar = self.nnFFNN(x_tensor)\n",
    "            y_hat = mean.item()\n",
    "            sigma2_a = torch.exp(logvar).item()\n",
    "        \n",
    "        # Determine which variance to use\n",
    "        sigma2_t = sigma2_a if y_hat < self.yq else self.epsilon\n",
    "        \n",
    "        # Compute heuristic value\n",
    "        h_val = self.h(self.alpha, y_hat, math.sqrt(sigma2_t))\n",
    "        return max(h_val, 0)\n",
    "    \n",
    "    def compute_metrics(self):\n",
    "        \"\"\"Compute suboptimality and optimality metrics\"\"\"\n",
    "        if not self.planner_costs:\n",
    "            return 0.0, 0.0\n",
    "        \n",
    "        # Calculate suboptimality (u_i)\n",
    "        suboptimalities = [\n",
    "            (y / y_star) - 1 \n",
    "            for y, y_star in zip(self.planner_costs, self.optimal_costs)\n",
    "            if y_star > 0  # Avoid division by zero\n",
    "        ]\n",
    "        avg_suboptimality = sum(suboptimalities) / len(suboptimalities) if suboptimalities else 0.0\n",
    "\n",
    "        # Calculate optimality rate (% tasks solved optimally)\n",
    "        optimality_rate = (self.optimality_counts / len(self.planner_costs)) * 100 if self.planner_costs else 0.0\n",
    "\n",
    "        return avg_suboptimality, optimality_rate\n",
    "    \n",
    "    def train_ffnn(self):\n",
    "        \"\"\"Trains FFNN on entire memory buffer\"\"\"\n",
    "        if len(self.memoryBuffer) < self.MiniBatchSize:\n",
    "            return\n",
    "        \n",
    "        optimizer = optim.Adam(self.nnFFNN.parameters())\n",
    "        criterion = nn.GaussianNLLLoss()\n",
    "        \n",
    "        # Convert memory buffer to tensors\n",
    "        x_data = torch.stack([torch.tensor(x, dtype=torch.float32) for x, _ in self.memoryBuffer])\n",
    "        y_data = torch.tensor([y for _, y in self.memoryBuffer], dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "        self.nnFFNN.train()\n",
    "        for _ in range(self.TrainIter):\n",
    "            # Shuffle and batch the data\n",
    "            permutation = torch.randperm(len(x_data))\n",
    "            for i in range(0, len(x_data), self.MiniBatchSize):\n",
    "                indices = permutation[i:i+self.MiniBatchSize]\n",
    "                x_batch, y_batch = x_data[indices], y_data[indices]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                mean, logvar = self.nnFFNN(x_batch)\n",
    "                loss = criterion(mean, y_batch, torch.exp(logvar))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "    def train_wunn(self):\n",
    "        \"\"\"Trains WUNN with early stopping condition\"\"\"\n",
    "        if len(self.memoryBuffer) < self.MiniBatchSize:\n",
    "            return False\n",
    "        \n",
    "        self.nnWUNN.train()\n",
    "        optimizer = optim.Adam(self.nnWUNN.parameters())\n",
    "        \n",
    "        early_stop = False\n",
    "        for iter in range(self.MaxTrainIter):\n",
    "            # Mini-batch training\n",
    "            batch = random.sample(self.memoryBuffer, self.MiniBatchSize)\n",
    "            total_loss = 0\n",
    "            \n",
    "            for x, y in batch:\n",
    "                x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "                y_tensor = torch.tensor([y], dtype=torch.float32)\n",
    "                \n",
    "                # Forward pass with multiple samples\n",
    "                preds = torch.stack([self.nnWUNN.forward_single(x_tensor) for _ in range(self.nnWUNN.S)])\n",
    "                \n",
    "                # Compute loss\n",
    "                log_likelihood = -F.mse_loss(preds.mean(), y_tensor)\n",
    "                kl_div = self.nnWUNN.fc1.kl_divergence() + self.nnWUNN.fc2.kl_divergence()\n",
    "                loss = self.beta * kl_div - log_likelihood\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            # Check early stopping condition periodically\n",
    "            if iter % 10 == 0:\n",
    "                self.nnWUNN.eval()\n",
    "                all_low_uncertainty = True\n",
    "                \n",
    "                # Check a subset of the buffer for efficiency\n",
    "                check_samples = min(100, len(self.memoryBuffer))\n",
    "                for x, _ in random.sample(self.memoryBuffer, check_samples):\n",
    "                    sigma2_e = self.nnWUNN.predict_sigma_e(x, 10)  # Smaller K for faster checking\n",
    "                    if sigma2_e >= self.kappa * self.epsilon:\n",
    "                        all_low_uncertainty = False\n",
    "                        break\n",
    "                \n",
    "                if all_low_uncertainty:\n",
    "                    print(f\"WUNN training stopped early (iteration {iter})\")\n",
    "                    early_stop = True\n",
    "                    break\n",
    "                \n",
    "                self.nnWUNN.train()\n",
    "        \n",
    "        return early_stop\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Main learning loop\"\"\"\n",
    "        # Initialize yq\n",
    "        self.yq = -np.inf\n",
    "        \n",
    "        for n in range(1, self.NumIter + 1):\n",
    "            print(f\"\\n=== Iteration {n}/{self.NumIter} ===\")\n",
    "            \n",
    "            # Update yq from memory buffer\n",
    "            if self.memoryBuffer:\n",
    "                costs = [y for _, y in self.memoryBuffer]\n",
    "                self.yq = np.quantile(costs, self.q)\n",
    "            print(f\"Current yq (q={self.q} quantile): {self.yq:.2f}, α: {self.alpha:.3f}, β: {self.beta:.3f}\")\n",
    "            \n",
    "            # Generate and solve tasks\n",
    "            numSolved = 0\n",
    "            for i in range(self.NumTasksPerIter):\n",
    "                T = self.generate_task()\n",
    "                if T is None:\n",
    "                    print(\"No task generated (low uncertainty)\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    start_time = time.time()\n",
    "                    plan = self.ida_star(\n",
    "                        T['s'],\n",
    "                        T['sg'],\n",
    "                        self.uncertainty_aware_heuristic,\n",
    "                        self.tmax,\n",
    "                        start_time\n",
    "                    )\n",
    "                    \n",
    "                    if plan:\n",
    "                        numSolved += 1\n",
    "                        plan_cost = len(plan) - 1  # Cost = steps taken\n",
    "                        optimal_cost = manhattan_distance(T['s'], T['sg'])\n",
    "                        \n",
    "                        # Record costs for metrics\n",
    "                        self.planner_costs.append(plan_cost)\n",
    "                        self.optimal_costs.append(optimal_cost)\n",
    "                        \n",
    "                        # Track optimal solutions\n",
    "                        if plan_cost == optimal_cost:\n",
    "                            self.optimality_counts += 1\n",
    "                        \n",
    "                        print(f\"✓ Solved task {i+1} (cost: {plan_cost}, optimal: {optimal_cost})\")\n",
    "                        \n",
    "                        # Add to memory buffer (most recent first)\n",
    "                        for state in reversed(plan[:-1]):  # Exclude goal state, reversed for recent first\n",
    "                            x = encode_15puzzle_state(state)\n",
    "                            y = manhattan_distance(state, T['sg'])\n",
    "                            self.memoryBuffer.appendleft((x, y))  # Add to front\n",
    "                \n",
    "                except TimeoutError:\n",
    "                    print(f\"⏳ Task {i+1} timed out\")\n",
    "                    continue\n",
    "            \n",
    "            # Update α and β based on solved tasks\n",
    "            if numSolved < self.NumTasksPerIterThresh:\n",
    "                self.alpha = max(self.alpha - self.delta, 0.5)\n",
    "                self.updateBeta = False\n",
    "                print(f\"Reduced α to {self.alpha:.3f} (only solved {numSolved} tasks)\")\n",
    "            else:\n",
    "                self.updateBeta = True\n",
    "            \n",
    "            # Train models\n",
    "            print(\"Training models...\")\n",
    "            self.train_ffnn()\n",
    "            early_stop = self.train_wunn()\n",
    "            \n",
    "            # Update β if conditions met\n",
    "            if self.updateBeta and not early_stop:\n",
    "                self.beta *= self.gamma\n",
    "                print(f\"Reduced β to {self.beta:.3f}\")\n",
    "            \n",
    "            # Compute and log metrics\n",
    "            avg_subopt, opt_rate = self.compute_metrics()\n",
    "            print(f\"Iteration {n} complete:\")\n",
    "            print(f\"  Solved {numSolved}/{self.NumTasksPerIter} tasks\")\n",
    "            print(f\"  Avg Suboptimality: {avg_subopt:.3f}\")\n",
    "            print(f\"  Optimality Rate: {opt_rate:.1f}%\")\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Define goal state and feature dimension\n",
    "    goal_state = list(range(16))\n",
    "    input_dim = len(encode_15puzzle_state(goal_state))\n",
    "    \n",
    "    # Algorithm parameters\n",
    "    params = {\n",
    "        'hidden_dim': 20,\n",
    "        'dropout_rate': 0.1,\n",
    "        'alpha0': 0.99,\n",
    "        'beta0': 0.05,\n",
    "        'epsilon': 1.0,\n",
    "        'delta': 0.01,\n",
    "        'kappa': 0.5,\n",
    "        'gamma': 0.9,\n",
    "        'q': 0.95,\n",
    "        'K': 50,\n",
    "        'NumIter': 30,\n",
    "        'NumTasksPerIter': 25,\n",
    "        'NumTasksPerIterThresh': 3,\n",
    "        'TrainIter': 200,\n",
    "        'MaxTrainIter': 1000,\n",
    "        'MiniBatchSize': 32,\n",
    "        'tmax': 30,  # seconds per task\n",
    "        'MemoryBufferMaxRecords': 5000\n",
    "    }\n",
    "    \n",
    "    # Initialize and run the algorithm\n",
    "    learner = LearnHeuristicPrac(input_dim, goal_state, params)\n",
    "    learner.run()\n",
    "\n",
    "    # Final metrics\n",
    "    avg_subopt, opt_rate = learner.compute_metrics()\n",
    "    print(f\"\\n=== Final Metrics ===\")\n",
    "    print(f\"Average Suboptimality: {avg_subopt:.3f}\")\n",
    "    print(f\"Optimality Rate: {opt_rate:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
