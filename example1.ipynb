{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e42e686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wf/809cg8f11bg1z8_9xt4wccnh0000gn/T/ipykernel_11542/4266355612.py:92: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  log_likelihood = -F.mse_loss(preds.mean(), y_tensor)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WUNN - Iteration [0/500], Loss: 80.9411\n",
      "WUNN - Iteration [100/500], Loss: 2.7428\n",
      "WUNN - Iteration [200/500], Loss: 1.9361\n",
      "WUNN - Iteration [300/500], Loss: 1.8385\n",
      "WUNN - Iteration [400/500], Loss: 1.8033\n",
      "Step 1: Checking state [4, 1, 2, 3, 0, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] - Epistemic uncertainty (σ²_e) = 2.4211, Epsilon (ϵ) = 1\n",
      "Step 1: Checking state [1, 0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] - Epistemic uncertainty (σ²_e) = 1.6194, Epsilon (ϵ) = 1\n",
      "Task generated with uncertainty σ²_e = 2.4211 ≥ ϵ = 1\n",
      "Generated Task: {'S': [4, 1, 2, 3, 0, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], 'E': np.float64(2.421100339200478), 's': [4, 1, 2, 3, 0, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], 'sg': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# --- Puzzle Environment Functions ---\n",
    "def encode_15puzzle_state(state):\n",
    "    encoded = np.zeros(16 * 2 * 4)\n",
    "    for tile in range(16):\n",
    "        idx = state.index(tile)\n",
    "        row, col = divmod(idx, 4)\n",
    "        encoded[tile * 8 + row] = 1\n",
    "        encoded[tile * 8 + 4 + col] = 1\n",
    "    return encoded\n",
    "\n",
    "def get_valid_moves(state):\n",
    "    zero_index = state.index(0)\n",
    "    row, col = divmod(zero_index, 4)\n",
    "    valid_moves = []\n",
    "    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    for dr, dc in directions:\n",
    "        new_row, new_col = row + dr, col + dc\n",
    "        if 0 <= new_row < 4 and 0 <= new_col < 4:\n",
    "            new_zero_index = new_row * 4 + new_col\n",
    "            new_state = state[:]\n",
    "            new_state[zero_index], new_state[new_zero_index] = new_state[new_zero_index], new_state[zero_index]\n",
    "            valid_moves.append(new_state)\n",
    "    return valid_moves\n",
    "\n",
    "# --- Bayesian Neural Network ---\n",
    "class BayesianLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, prior_std=2.0):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # Weight and bias parameters\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features).normal_(0, 0.1))\n",
    "        self.weight_logvar = nn.Parameter(torch.Tensor(out_features, in_features).fill_(-2))\n",
    "\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(out_features).normal_(0, 0.1))\n",
    "        self.bias_logvar = nn.Parameter(torch.Tensor(out_features).fill_(-2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Local reparameterization trick:\n",
    "        # Compute mean and variance of output activation\n",
    "        weight_var = torch.exp(self.weight_logvar)\n",
    "        bias_var = torch.exp(self.bias_logvar)\n",
    "\n",
    "        # Mean and variance of linear output\n",
    "        mu_out = F.linear(x, self.weight_mu, self.bias_mu)\n",
    "        var_out = F.linear(x.pow(2), weight_var, bias_var)\n",
    "\n",
    "        # Sample from activation distribution\n",
    "        eps = torch.randn_like(mu_out)\n",
    "        out = mu_out + torch.sqrt(var_out + 1e-8) * eps  # add small epsilon for numerical stability\n",
    "        return out\n",
    "\n",
    "    def kl_divergence(self):\n",
    "        # KL divergence between variational posterior and prior\n",
    "        kl = 0.5 * (self.weight_mu.pow(2) + torch.exp(self.weight_logvar) - self.weight_logvar - 1).sum()\n",
    "        kl += 0.5 * (self.bias_mu.pow(2) + torch.exp(self.bias_logvar) - self.bias_logvar - 1).sum()\n",
    "        return kl\n",
    "\n",
    "\n",
    "class WUNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=20, S=5, C=1, prior_std=1.0):\n",
    "        super().__init__()\n",
    "        self.S = S\n",
    "        self.C = C\n",
    "        self.fc1 = BayesianLinear(input_dim, hidden_dim, prior_std)\n",
    "        self.fc2 = BayesianLinear(hidden_dim, 1, prior_std)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.01)\n",
    "\n",
    "    def forward_single(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "    def predict_sigma_e(self, x, K):\n",
    "        self.eval()\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            outputs = [self.forward_single(x_tensor).item() for _ in range(K)]\n",
    "        return np.var(outputs)\n",
    "\n",
    "    def elbo_loss(self, x, y, beta=0.05):\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "        y_tensor = torch.tensor([y], dtype=torch.float32)\n",
    "        preds = torch.stack([self.forward_single(x_tensor) for _ in range(self.S)])\n",
    "        log_likelihood = -F.mse_loss(preds.mean(), y_tensor)\n",
    "        kl_div = self.fc1.kl_divergence() + self.fc2.kl_divergence()\n",
    "        return beta * kl_div + (-log_likelihood)\n",
    "\n",
    "    def sample_weighted_batch(self, memory_buffer, batch_size, kappa_epsilon, K):\n",
    "        weights = []\n",
    "        for x, _ in memory_buffer:\n",
    "            sigma2 = self.predict_sigma_e(x, K)\n",
    "            weights.append(sigma2 + 1e-6)  # More numerically stable\n",
    "        weights = np.array(weights)\n",
    "        weights /= weights.sum()\n",
    "        indices = np.random.choice(len(memory_buffer), size=min(batch_size, len(memory_buffer)),\n",
    "                                   replace=False, p=weights)\n",
    "        return [memory_buffer[i] for i in indices]\n",
    "\n",
    "    def train_model(self, memory_buffer, max_iter=500, batch_size=10, kappa_epsilon=0.1, K=5):\n",
    "        self.train()\n",
    "        for iteration in range(max_iter):\n",
    "            batch = self.sample_weighted_batch(memory_buffer, batch_size, kappa_epsilon, K)\n",
    "            for x, y in batch:\n",
    "                loss = self.elbo_loss(x, y)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            if iteration % 100 == 0:\n",
    "                print(f\"WUNN - Iteration [{iteration}/{max_iter}], Loss: {loss.item():.4f}\")\n",
    "            if all(self.predict_sigma_e(x, K=5) < kappa_epsilon for x, _ in memory_buffer):\n",
    "                print(\"Convergence reached. Stopping training.\")\n",
    "                break\n",
    "\n",
    "    \n",
    "# --- Task Generator ---\n",
    "def GenerateTaskPrac(nnWUNN, epsilon, MaxSteps, K, Erev, feature, sg):\n",
    "    s_prime = sg\n",
    "    numSteps = 0\n",
    "    s_prev = None\n",
    "    while numSteps < MaxSteps:\n",
    "        numSteps += 1\n",
    "        states = {}\n",
    "\n",
    "        for s in Erev(s_prime):\n",
    "            if s_prev is not None and s == s_prev:\n",
    "                continue\n",
    "            x = feature(s)\n",
    "            sigma2_e = nnWUNN.predict_sigma_e(x, K)\n",
    "            states[tuple(s)] = sigma2_e\n",
    "            print(f\"Step {numSteps}: Checking state {s} - Epistemic uncertainty (σ²_e) = {sigma2_e:.4f}, Epsilon (ϵ) = {epsilon}\")\n",
    "\n",
    "        if not states:\n",
    "            return None\n",
    "\n",
    "        keys = list(states.keys())\n",
    "        values = np.array([states[k] for k in keys])\n",
    "        probs = np.exp(values) / np.sum(np.exp(values))\n",
    "        s = list(keys[np.random.choice(len(keys), p=probs)])\n",
    "\n",
    "        if states[tuple(s)] >= epsilon:\n",
    "            T = {\n",
    "                \"S\": s,\n",
    "                \"E\": states[tuple(s)],\n",
    "                \"s\": s,\n",
    "                \"sg\": sg\n",
    "            }\n",
    "\n",
    "            # The value of O and C?\n",
    "\n",
    "            print(f\"Task generated with uncertainty σ²_e = {states[tuple(s)]:.4f} ≥ ϵ = {epsilon}\")\n",
    "            return T\n",
    "\n",
    "        s_prev = s_prime\n",
    "        s_prime = s\n",
    "\n",
    "    return None\n",
    "\n",
    "# --- Example Execution ---\n",
    "goal_state = list(range(16))\n",
    "feature = lambda s: np.array(encode_15puzzle_state(s))\n",
    "Erev = get_valid_moves\n",
    "input_dim = len(feature(goal_state))\n",
    "nnWUNN = WUNN(input_dim)\n",
    "\n",
    "# Scrambling utility\n",
    "def scramble(state, steps=10):\n",
    "    s = state[:]\n",
    "    for _ in range(steps):\n",
    "        s = random.choice(get_valid_moves(s))\n",
    "    return s\n",
    "\n",
    "# Value function (distance to goal)\n",
    "def manhattan_distance(state, goal_state):\n",
    "    total = 0\n",
    "    for i in range(1, 16):\n",
    "        curr_idx = state.index(i)\n",
    "        goal_idx = goal_state.index(i)\n",
    "        curr_row, curr_col = divmod(curr_idx, 4)\n",
    "        goal_row, goal_col = divmod(goal_idx, 4)\n",
    "        total += abs(curr_row - goal_row) + abs(curr_col - goal_col)\n",
    "    return total\n",
    "\n",
    "# Create training data\n",
    "memory_buffer = [\n",
    "    (feature(scramble(goal_state, steps=np.random.randint(1, 20))), manhattan_distance(scramble(goal_state, 1), goal_state))\n",
    "    for _ in range(200)\n",
    "]\n",
    "\n",
    "# Train WUNN\n",
    "nnWUNN.train_model(memory_buffer)\n",
    "\n",
    "# Generate task\n",
    "task = GenerateTaskPrac(nnWUNN, epsilon=1, MaxSteps=1000, K=100, Erev=Erev, feature=feature, sg=goal_state)\n",
    "print(\"Generated Task:\", task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c05d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FFNN ---\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=20, dropout_rate=0.025):\n",
    "        super(FFNN, self).__init__()\n",
    "        \n",
    "        # Define the layers of the FFNN\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)  # Output layer (predicts the mean y)\n",
    "        self.fc2_logvar = nn.Linear(hidden_dim, 1)  # Output layer for aleatoric uncertainty (log variance)\n",
    "        \n",
    "        # Dropout layer for aleatoric uncertainty estimation\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # He Normal Initialization for the layers\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.fc2.weight, mode='fan_in', nonlinearity='linear')\n",
    "        nn.init.kaiming_normal_(self.fc2_logvar.weight, mode='fan_in', nonlinearity='linear')\n",
    "\n",
    "        # Initialize biases to 0\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "        nn.init.zeros_(self.fc2_logvar.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        - Apply first hidden layer with ReLU activation\n",
    "        - Apply dropout for aleatoric uncertainty estimation\n",
    "        - Apply second hidden layer (for cost-to-goal prediction and uncertainty estimation)\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))  # Apply ReLU activation to the first hidden layer\n",
    "        \n",
    "        # Apply dropout to introduce aleatoric uncertainty\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Predict the cost-to-goal (mean value)\n",
    "        mean_output = self.fc2(x)\n",
    "        \n",
    "        # Predict the log-variance (for aleatoric uncertainty)\n",
    "        logvar_output = self.fc2_logvar(x)\n",
    "        \n",
    "        return mean_output, logvar_output\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict the cost-to-goal (mean) and aleatoric uncertainty (log variance).\n",
    "        \"\"\"\n",
    "        mean, logvar = self.forward(x)\n",
    "        return mean, torch.exp(logvar)  # Return exp of log variance to get variance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
